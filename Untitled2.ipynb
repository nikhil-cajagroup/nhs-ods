{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6de78112-baa6-4f5b-9257-016ae99ca5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bronze root: C:\\Users\\NikhilYadav\\Desktop\\NHS ODS\\bronze\\ods\n",
      "Discovering all Role IDs from /roles ...\n",
      "Discovered 202 roles\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b60815e7cd6949f8860549eeaf672993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SELECTED ROLES (202):   0%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Crawling RO101 (48048 orgs)\n",
      "→ Crawling RO102 (45 orgs)\n",
      "→ Crawling RO103 (41 orgs)\n",
      "→ Crawling RO104 (26157 orgs)\n",
      "→ Crawling RO105 (12 orgs)\n",
      "→ Crawling RO106 (2 orgs)\n",
      "→ Crawling RO107 (12 orgs)\n",
      "→ Crawling RO108 (1035 orgs)\n",
      "→ Crawling RO109 (108 orgs)\n",
      "→ Crawling RO11 (0 orgs)\n",
      "→ Crawling RO110 (11981 orgs)\n",
      "→ Crawling RO111 (106 orgs)\n",
      "→ Crawling RO114 (348 orgs)\n",
      "→ Crawling RO116 (14 orgs)\n",
      "→ Crawling RO117 (30 orgs)\n",
      "→ Crawling RO119 (418 orgs)\n",
      "→ Crawling RO12 (0 orgs)\n",
      "→ Crawling RO122 (222 orgs)\n",
      "→ Crawling RO123 (37 orgs)\n",
      "→ Crawling RO126 (18 orgs)\n",
      "→ Crawling RO128 (10 orgs)\n",
      "→ Crawling RO131 (47 orgs)\n",
      "→ Crawling RO132 (149 orgs)\n",
      "→ Crawling RO134 (11 orgs)\n",
      "→ Crawling RO136 (41 orgs)\n",
      "→ Crawling RO137 (5 orgs)\n",
      "→ Crawling RO138 (1 orgs)\n",
      "→ Crawling RO140 (9 orgs)\n",
      "→ Crawling RO141 (421 orgs)\n",
      "→ Crawling RO142 (7 orgs)\n",
      "→ Crawling RO144 (22 orgs)\n",
      "→ Crawling RO146 (5 orgs)\n",
      "→ Crawling RO147 (20 orgs)\n",
      "→ Crawling RO148 (915 orgs)\n",
      "→ Crawling RO149 (65 orgs)\n",
      "→ Crawling RO15 (0 orgs)\n",
      "→ Crawling RO150 (5 orgs)\n",
      "→ Crawling RO153 (1 orgs)\n",
      "→ Crawling RO154 (6 orgs)\n",
      "→ Crawling RO155 (5 orgs)\n",
      "→ Crawling RO157 (17745 orgs)\n",
      "→ Crawling RO158 (6 orgs)\n",
      "→ Crawling RO159 (10 orgs)\n",
      "→ Crawling RO161 (202 orgs)\n",
      "→ Crawling RO162 (16 orgs)\n",
      "→ Crawling RO166 (4465 orgs)\n",
      "→ Crawling RO167 (9260 orgs)\n",
      "→ Crawling RO168 (81 orgs)\n",
      "→ Crawling RO169 (4 orgs)\n",
      "→ Crawling RO171 (528 orgs)\n",
      "→ Crawling RO172 (5670 orgs)\n",
      "→ Crawling RO173 (284 orgs)\n",
      "→ Crawling RO175 (101 orgs)\n",
      "→ Crawling RO176 (19156 orgs)\n",
      "→ Crawling RO177 (15270 orgs)\n",
      "→ Crawling RO179 (395 orgs)\n",
      "→ Crawling RO180 (14693 orgs)\n",
      "→ Crawling RO181 (5904 orgs)\n",
      "→ Crawling RO182 (16229 orgs)\n",
      "→ Crawling RO185 (17 orgs)\n",
      "→ Crawling RO189 (39 orgs)\n",
      "→ Crawling RO190 (14 orgs)\n",
      "→ Crawling RO191 (130 orgs)\n",
      "→ Crawling RO197 (543 orgs)\n",
      "→ Crawling RO198 (48199 orgs)\n",
      "→ Crawling RO200 (1 orgs)\n",
      "→ Crawling RO209 (10 orgs)\n",
      "→ Crawling RO21 (0 orgs)\n",
      "→ Crawling RO210 (42 orgs)\n",
      "→ Crawling RO211 (0 orgs)\n",
      "→ Crawling RO212 (24 orgs)\n",
      "→ Crawling RO213 (36 orgs)\n",
      "→ Crawling RO214 (147 orgs)\n",
      "→ Crawling RO215 (0 orgs)\n",
      "→ Crawling RO216 (12 orgs)\n",
      "→ Crawling RO217 (2 orgs)\n",
      "→ Crawling RO218 (0 orgs)\n",
      "→ Crawling RO22 (0 orgs)\n",
      "→ Crawling RO221 (29143 orgs)\n",
      "→ Crawling RO222 (570 orgs)\n",
      "→ Crawling RO223 (0 orgs)\n",
      "→ Crawling RO227 (1233 orgs)\n",
      "→ Crawling RO228 (33 orgs)\n",
      "→ Crawling RO229 (0 orgs)\n",
      "→ Crawling RO230 (3 orgs)\n",
      "→ Crawling RO231 (18 orgs)\n",
      "→ Crawling RO232 (15 orgs)\n",
      "→ Crawling RO233 (46 orgs)\n",
      "→ Crawling RO234 (448 orgs)\n",
      "→ Crawling RO235 (97 orgs)\n",
      "→ Crawling RO236 (49 orgs)\n",
      "→ Crawling RO24 (0 orgs)\n",
      "→ Crawling RO246 (0 orgs)\n",
      "→ Crawling RO247 (0 orgs)\n",
      "→ Crawling RO248 (0 orgs)\n",
      "→ Crawling RO249 (0 orgs)\n",
      "→ Crawling RO25 (0 orgs)\n",
      "→ Crawling RO250 (0 orgs)\n",
      "→ Crawling RO251 (0 orgs)\n",
      "→ Crawling RO252 (0 orgs)\n",
      "→ Crawling RO255 (0 orgs)\n",
      "→ Crawling RO256 (0 orgs)\n",
      "→ Crawling RO257 (0 orgs)\n",
      "→ Crawling RO258 (0 orgs)\n",
      "→ Crawling RO259 (0 orgs)\n",
      "→ Crawling RO260 (0 orgs)\n",
      "→ Crawling RO261 (118 orgs)\n",
      "→ Crawling RO262 (0 orgs)\n",
      "→ Crawling RO266 (0 orgs)\n",
      "→ Crawling RO267 (0 orgs)\n",
      "→ Crawling RO268 (0 orgs)\n",
      "→ Crawling RO269 (0 orgs)\n",
      "→ Crawling RO270 (0 orgs)\n",
      "→ Crawling RO272 (1386 orgs)\n",
      "→ Crawling RO273 (0 orgs)\n",
      "→ Crawling RO274 (0 orgs)\n",
      "→ Crawling RO275 (0 orgs)\n",
      "→ Crawling RO276 (0 orgs)\n",
      "→ Crawling RO277 (0 orgs)\n",
      "→ Crawling RO279 (0 orgs)\n",
      "→ Crawling RO280 (3035 orgs)\n",
      "→ Crawling RO281 (0 orgs)\n",
      "→ Crawling RO282 (0 orgs)\n",
      "→ Crawling RO283 (0 orgs)\n",
      "→ Crawling RO284 (0 orgs)\n",
      "→ Crawling RO285 (0 orgs)\n",
      "→ Crawling RO286 (0 orgs)\n",
      "→ Crawling RO287 (0 orgs)\n",
      "→ Crawling RO288 (0 orgs)\n",
      "→ Crawling RO289 (0 orgs)\n",
      "→ Crawling RO29 (0 orgs)\n",
      "→ Crawling RO290 (0 orgs)\n",
      "→ Crawling RO291 (0 orgs)\n",
      "→ Crawling RO292 (0 orgs)\n",
      "→ Crawling RO293 (0 orgs)\n",
      "→ Crawling RO294 (0 orgs)\n",
      "→ Crawling RO296 (0 orgs)\n",
      "→ Crawling RO297 (0 orgs)\n",
      "→ Crawling RO298 (0 orgs)\n",
      "→ Crawling RO299 (0 orgs)\n",
      "→ Crawling RO30 (0 orgs)\n",
      "→ Crawling RO300 (0 orgs)\n",
      "→ Crawling RO301 (0 orgs)\n",
      "→ Crawling RO302 (0 orgs)\n",
      "→ Crawling RO303 (0 orgs)\n",
      "→ Crawling RO304 (0 orgs)\n",
      "→ Crawling RO305 (0 orgs)\n",
      "→ Crawling RO306 (0 orgs)\n",
      "→ Crawling RO307 (0 orgs)\n",
      "→ Crawling RO308 (0 orgs)\n",
      "→ Crawling RO309 (0 orgs)\n",
      "→ Crawling RO31 (0 orgs)\n",
      "→ Crawling RO310 (0 orgs)\n",
      "→ Crawling RO311 (0 orgs)\n",
      "→ Crawling RO312 (0 orgs)\n",
      "→ Crawling RO313 (0 orgs)\n",
      "→ Crawling RO314 (0 orgs)\n",
      "→ Crawling RO315 (321 orgs)\n",
      "→ Crawling RO316 (0 orgs)\n",
      "→ Crawling RO317 (0 orgs)\n",
      "→ Crawling RO318 (0 orgs)\n",
      "→ Crawling RO319 (0 orgs)\n",
      "→ Crawling RO320 (0 orgs)\n",
      "→ Crawling RO321 (0 orgs)\n",
      "→ Crawling RO322 (4 orgs)\n",
      "→ Crawling RO323 (0 orgs)\n",
      "→ Crawling RO324 (0 orgs)\n",
      "→ Crawling RO325 (0 orgs)\n",
      "→ Crawling RO326 (0 orgs)\n",
      "→ Crawling RO327 (0 orgs)\n",
      "→ Crawling RO328 (7 orgs)\n",
      "→ Crawling RO33 (0 orgs)\n",
      "→ Crawling RO34 (0 orgs)\n",
      "→ Crawling RO35 (0 orgs)\n",
      "→ Crawling RO36 (0 orgs)\n",
      "→ Crawling RO37 (0 orgs)\n",
      "→ Crawling RO38 (0 orgs)\n",
      "→ Crawling RO39 (0 orgs)\n",
      "→ Crawling RO40 (0 orgs)\n",
      "→ Crawling RO57 (0 orgs)\n",
      "→ Crawling RO59 (0 orgs)\n",
      "→ Crawling RO60 (0 orgs)\n",
      "→ Crawling RO65 (0 orgs)\n",
      "→ Crawling RO67 (0 orgs)\n",
      "→ Crawling RO7 (0 orgs)\n",
      "→ Crawling RO71 (0 orgs)\n",
      "→ Crawling RO72 (0 orgs)\n",
      "→ Crawling RO76 (0 orgs)\n",
      "→ Crawling RO80 (0 orgs)\n",
      "→ Crawling RO82 (0 orgs)\n",
      "→ Crawling RO83 (0 orgs)\n",
      "→ Crawling RO87 (0 orgs)\n",
      "→ Crawling RO88 (1741 orgs)\n",
      "→ Crawling RO89 (18 orgs)\n",
      "→ Crawling RO90 (105 orgs)\n",
      "→ Crawling RO91 (26 orgs)\n",
      "→ Crawling RO92 (313 orgs)\n",
      "→ Crawling RO93 (10 orgs)\n",
      "→ Crawling RO94 (120 orgs)\n",
      "→ Crawling RO96 (6545 orgs)\n",
      "→ Crawling RO98 (343 orgs)\n",
      "→ Crawling RO99 (2185 orgs)\n",
      "[OK] Baseline complete → C:\\Users\\NikhilYadav\\Desktop\\NHS ODS\\bronze\\ods\\release_date=2025-09-23\\source=ord\\release_type=api_baseline\\dataset=roles\n",
      "Org files: 282872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === NHS ODS Bronze (Jupyter) — ALL-IN-ONE (minimal progress UI) ===\n",
    "# - ROLE_IDS=None -> auto-discovers ALL Role IDs from /roles (no 406)\n",
    "# - Spec-compliant ORD calls, _format=json, 1-based Offset\n",
    "# - Single overall tqdm line showing only % (no page spam); per-role bars hidden\n",
    "# - Checkpoints (resume per role), dedupe across roles\n",
    "# - Incremental sync (LastChangeDate)\n",
    "# - Robust flattener: orgs + addresses + roles + rels + successors\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "import os, json, time, math, re, hashlib, requests\n",
    "from datetime import datetime, timedelta, timezone, date\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Optional, Tuple\n",
    "from urllib.parse import urlparse\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm  # notebook-friendly\n",
    "\n",
    "# ------------------- USER SETTINGS -------------------\n",
    "BRONZE_ROOT = Path(r\"C:\\Users\\NikhilYadav\\Desktop\\NHS ODS\\bronze\\ods\")\n",
    "ORD_BASE    = \"https://directory.spineservices.nhs.uk/ORD/2-0-0\"\n",
    "RATE_LIMIT_RPS = 4\n",
    "PAGE_LIMIT = 1000\n",
    "\n",
    "# Put role IDs here OR set to None to auto-discover *all* role IDs from /roles\n",
    "ROLE_IDS: Optional[List[str]] = None     # e.g. [\"RO177\",\"RO98\"] or None for ALL roles\n",
    "\n",
    "# If PrimaryRoleId is rejected, fall back to Roles=\n",
    "USE_ROLES_PARAM_IF_NEEDED = True\n",
    "\n",
    "# Create tidy extracts at the end\n",
    "MAKE_CLEAN_EXTRACTS = True\n",
    "\n",
    "# ------------------- MINIMAL PROGRESS BAR STYLE -------------------\n",
    "TQDM_BAR_OVERALL = \"{desc}: {percentage:3.0f}%\"\n",
    "TQDM_KW = dict(\n",
    "    bar_format=TQDM_BAR_OVERALL,\n",
    "    dynamic_ncols=True,\n",
    "    mininterval=1.5,\n",
    "    smoothing=0.2,\n",
    "    ascii=None,\n",
    ")\n",
    "\n",
    "# ------------------- UTILS -------------------\n",
    "def now_utc_iso() -> str:\n",
    "    return datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "def ensure_dir(p: Path) -> None:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def write_json(path: Path, obj: Any) -> None:\n",
    "    ensure_dir(path.parent)\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "def read_json(path: Path, default=None):\n",
    "    if not path.exists():\n",
    "        return default\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# watermarks\n",
    "def wm_path() -> Path: return BRONZE_ROOT / \"_watermarks.json\"\n",
    "def get_wm() -> Dict[str, Any]: return read_json(wm_path(), default={}) or {}\n",
    "def set_wm(key: str, val: Any) -> None:\n",
    "    wm = get_wm(); wm[key] = val; write_json(wm_path(), wm)\n",
    "\n",
    "# checkpoints (resume per role)\n",
    "CKPT_DIR = BRONZE_ROOT / \"_checkpoints\"\n",
    "ensure_dir(CKPT_DIR)\n",
    "def load_ckpt(role_id: str) -> int:\n",
    "    j = read_json(CKPT_DIR / f\"{role_id}.json\", {})\n",
    "    return int(j.get(\"next_offset\", 1))  # 1-based\n",
    "def save_ckpt(role_id: str, next_offset: int):\n",
    "    write_json(CKPT_DIR / f\"{role_id}.json\", {\"next_offset\": int(next_offset), \"saved_at\": now_utc_iso()})\n",
    "\n",
    "# dedupe set (resume-aware)\n",
    "def preload_seen_orgs() -> set:\n",
    "    seen=set()\n",
    "    for p in BRONZE_ROOT.glob(\"release_date=*/source=ord/**/chunks/org_*.json\"):\n",
    "        try:\n",
    "            oid = p.stem.replace(\"org_\",\"\")\n",
    "            seen.add(oid)\n",
    "        except: \n",
    "            pass\n",
    "    return seen\n",
    "\n",
    "# ------------------- HTTP (spec-compliant) -------------------\n",
    "session = requests.Session()\n",
    "session.headers.update({\"User-Agent\": \"ods-bronze/1.5\"})  # friendly UA only\n",
    "\n",
    "def rate_sleep():\n",
    "    time.sleep(1.0 / max(RATE_LIMIT_RPS, 1))\n",
    "\n",
    "def ord_request(url: str, params: Dict[str, Any]) -> requests.Response:\n",
    "    q = dict(params)\n",
    "    q[\"_format\"] = \"json\"  # lowercase only\n",
    "    rate_sleep()\n",
    "    return session.get(url, params=q, timeout=60, allow_redirects=True)\n",
    "\n",
    "def ord_get_json(url: str, params: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    r = ord_request(url, params)\n",
    "    if not (200 <= r.status_code < 300):\n",
    "        raise RuntimeError(f\"ORD GET failed {r.status_code}. URL: {r.url}\\nBody: {r.text}\")\n",
    "    return r.json()\n",
    "\n",
    "def ord_get_full_org(link: str, retries=3, backoff=1.5) -> Dict[str, Any]:\n",
    "    if \"_format=\" not in link:\n",
    "        sep = \"&\" if \"?\" in link else \"?\"\n",
    "        link = f\"{link}{sep}_format=json\"\n",
    "    for attempt in range(1, retries+1):\n",
    "        rate_sleep()\n",
    "        r = session.get(link, timeout=60, allow_redirects=True)\n",
    "        if 200 <= r.status_code < 300:\n",
    "            return r.json()\n",
    "        if attempt == retries:\n",
    "            raise RuntimeError(f\"ORD org GET failed {r.status_code}. URL: {link}\\nBody: {r.text}\")\n",
    "        time.sleep(backoff**attempt)\n",
    "\n",
    "# ------------------- ROLE DISCOVERY (avoids 406) -------------------\n",
    "def discover_role_ids() -> List[str]:\n",
    "    \"\"\"Pull list of Role IDs from /roles (JSON first, fallback to XML).\"\"\"\n",
    "    roles_url = f\"{ORD_BASE.rstrip('/')}/roles\"\n",
    "    # Try JSON\n",
    "    try:\n",
    "        payload = ord_get_json(roles_url, params={})\n",
    "        ids=set()\n",
    "        def walk(x):\n",
    "            if isinstance(x, dict):\n",
    "                if \"id\" in x and isinstance(x[\"id\"], str) and x[\"id\"].upper().startswith(\"RO\"):\n",
    "                    ids.add(x[\"id\"].upper())\n",
    "                for v in x.values(): walk(v)\n",
    "            elif isinstance(x, list):\n",
    "                for it in x: walk(it)\n",
    "        walk(payload)\n",
    "        if ids:\n",
    "            return sorted(ids)\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Fallback: XML\n",
    "    r = ord_request(roles_url, params={\"_format\":\"xml\"})\n",
    "    r.raise_for_status()\n",
    "    txt = r.text\n",
    "    ids = sorted(set(re.findall(r\">\\s*(RO\\d+)\\s*<\", txt, flags=re.IGNORECASE)))\n",
    "    return [i.upper() for i in ids]\n",
    "\n",
    "# ------------------- TOTAL COUNT DISCOVERY -------------------\n",
    "def get_total_for_role(search_url: str, role_id: str, use_roles_param: bool) -> Tuple[Optional[int], Dict[str, Any]]:\n",
    "    \"\"\"Small page to read X-Total-Count; returns (total_or_None, params_used).\"\"\"\n",
    "    # Try PrimaryRoleId\n",
    "    params = {\"PrimaryRoleId\": role_id, \"Limit\": 1, \"Offset\": 1}\n",
    "    r = ord_request(search_url, params)\n",
    "    if 200 <= r.status_code < 300:\n",
    "        total = r.headers.get(\"X-Total-Count\")\n",
    "        return (int(total) if total and total.isdigit() else None, {\"PrimaryRoleId\": role_id})\n",
    "    # Fallback to Roles\n",
    "    if use_roles_param:\n",
    "        params = {\"Roles\": role_id, \"Limit\": 1, \"Offset\": 1}\n",
    "        r = ord_request(search_url, params)\n",
    "        if 200 <= r.status_code < 300:\n",
    "            total = r.headers.get(\"X-Total-Count\")\n",
    "            return (int(total) if total and total.isdigit() else None, {\"Roles\": role_id})\n",
    "        raise RuntimeError(f\"Unable to get total for {role_id}. URL: {r.url}\\nBody: {r.text}\")\n",
    "    raise RuntimeError(f\"Unable to get total for {role_id}. URL: {r.url}\\nBody: {r.text}\")\n",
    "\n",
    "# ------------------- BASELINE (minimal progress + resume + dedupe) -------------------\n",
    "def baseline_roles_with_progress(role_ids: List[str]) -> Path:\n",
    "    ensure_dir(BRONZE_ROOT)\n",
    "    release_date = datetime.now(timezone.utc).strftime(\"%Y-%m-%d\")\n",
    "    search_url   = ORD_BASE.rstrip(\"/\") + \"/organisations\"\n",
    "\n",
    "    base_dir  = BRONZE_ROOT / f\"release_date={release_date}\" / \"source=ord\" / \"release_type=api_baseline\" / \"dataset=roles\"\n",
    "    chunk_dir = base_dir / \"chunks\"\n",
    "    ensure_dir(chunk_dir)\n",
    "\n",
    "    # totals & plan\n",
    "    role_plans = []\n",
    "    grand_total = 0\n",
    "    totals_known = True\n",
    "    for rid in role_ids:\n",
    "        total, base_params = get_total_for_role(search_url, rid, USE_ROLES_PARAM_IF_NEEDED)\n",
    "        role_plans.append({\"role\": rid, \"total\": total, \"base_params\": base_params})\n",
    "        if total is None: totals_known = False\n",
    "        else: grand_total += total\n",
    "\n",
    "    manifest = {\n",
    "        \"api\": ORD_BASE, \"release_date\": release_date, \"release_type\": \"api_baseline\",\n",
    "        \"downloaded_at_utc\": now_utc_iso(), \"roles\": role_ids,\n",
    "        \"role_plans\": role_plans,\n",
    "        \"summary_chunks\": [], \"org_records\": []\n",
    "    }\n",
    "\n",
    "    # Overall progress (single, minimal line)\n",
    "    overall = tqdm(\n",
    "        total=grand_total if totals_known else None,\n",
    "        unit=\"org\",\n",
    "        desc=f\"SELECTED ROLES ({len(role_ids)})\",\n",
    "        leave=True,\n",
    "        **TQDM_KW\n",
    "    )\n",
    "\n",
    "    # Dedupe set\n",
    "    SEEN_ORGS = preload_seen_orgs()\n",
    "\n",
    "    # helper to page\n",
    "    def page_once(params: Dict[str, Any], offset: int, limit: int) -> Dict[str, Any]:\n",
    "        p = dict(params)\n",
    "        p[\"Limit\"]  = limit\n",
    "        p[\"Offset\"] = max(1, offset)  # 1-based\n",
    "        return ord_get_json(search_url, p)\n",
    "\n",
    "    # Crawl each role\n",
    "    for plan in role_plans:\n",
    "        rid = plan[\"role\"]\n",
    "        total = plan[\"total\"]\n",
    "        base_params = plan[\"base_params\"]\n",
    "\n",
    "        # One-line breadcrumb per role (non-spammy)\n",
    "        tqdm.write(f\"→ Crawling {rid} ({total if total is not None else 'unknown'} orgs)\")\n",
    "\n",
    "        # Hidden per-role bar to keep code consistent (no output)\n",
    "        per_role = tqdm(\n",
    "            total=total if total is not None else None,\n",
    "            unit=\"org\",\n",
    "            desc=f\"Role {rid}\",\n",
    "            leave=False,\n",
    "            disable=True,   # hide per-role bars\n",
    "            **TQDM_KW\n",
    "        )\n",
    "\n",
    "        offset = load_ckpt(rid)  # resume point (1-based)\n",
    "        while True:\n",
    "            try:\n",
    "                data = page_once(base_params, offset, PAGE_LIMIT)\n",
    "            except RuntimeError as e:\n",
    "                if \"PrimaryRoleId\" in base_params and USE_ROLES_PARAM_IF_NEEDED:\n",
    "                    base_params = {\"Roles\": rid}\n",
    "                    data = page_once(base_params, offset, PAGE_LIMIT)\n",
    "                else:\n",
    "                    per_role.close(); overall.close()\n",
    "                    raise\n",
    "\n",
    "            orgs = data.get(\"Organisations\", []) or []\n",
    "            if not orgs:\n",
    "                per_role.close()\n",
    "                save_ckpt(rid, 1)  # reset for next run\n",
    "                break\n",
    "\n",
    "            # save summary page\n",
    "            chunk_name = f\"search_{rid}_{offset:09d}.json\"\n",
    "            write_json(chunk_dir / chunk_name, data)\n",
    "            manifest[\"summary_chunks\"].append({\"role\": rid, \"file\": chunk_name, \"count\": len(orgs)})\n",
    "\n",
    "            # fetch each full record (dedupe on OrgId from summary)\n",
    "            for rec in orgs:\n",
    "                link = rec.get(\"OrgLink\"); oid = rec.get(\"OrgId\")\n",
    "                if not link or not oid:\n",
    "                    overall.update(1)\n",
    "                    continue\n",
    "                if oid in SEEN_ORGS:\n",
    "                    overall.update(1)\n",
    "                    continue\n",
    "                try:\n",
    "                    full = ord_get_full_org(link)\n",
    "                    ofile = f\"org_{oid}.json\"\n",
    "                    write_json(chunk_dir / ofile, full)\n",
    "                    manifest[\"org_records\"].append({\"org_id\": oid, \"file\": ofile})\n",
    "                    SEEN_ORGS.add(oid)\n",
    "                except Exception as ex:\n",
    "                    manifest.setdefault(\"errors\", []).append({\"org\": oid, \"error\": str(ex)})\n",
    "                finally:\n",
    "                    overall.update(1)\n",
    "\n",
    "            # next page (1, 1001, 2001, ...)\n",
    "            offset += PAGE_LIMIT\n",
    "            save_ckpt(rid, offset)\n",
    "\n",
    "            # stop at short page\n",
    "            if len(orgs) < PAGE_LIMIT:\n",
    "                per_role.close()\n",
    "                save_ckpt(rid, 1)\n",
    "                break\n",
    "\n",
    "    overall.close()\n",
    "    write_json(base_dir / \"_manifest.json\", manifest)\n",
    "\n",
    "    # watermarks\n",
    "    set_wm(\"ord_api_baseline_date\", release_date)\n",
    "    set_wm(\"ord_last_change_date\", (datetime.now(timezone.utc) - timedelta(days=1)).strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "    print(f\"[OK] Baseline complete → {base_dir}\")\n",
    "    print(\"Org files:\", len(manifest[\"org_records\"]))\n",
    "    return base_dir\n",
    "\n",
    "# ------------------- INCREMENTAL SYNC (same minimal bar) -------------------\n",
    "def incremental_sync_with_progress():\n",
    "    wm = get_wm()\n",
    "    since = wm.get(\"ord_last_change_date\")\n",
    "    assert since, \"No watermark found. Run baseline first.\"\n",
    "\n",
    "    sync_date = datetime.now(timezone.utc).strftime(\"%Y-%m-%d\")\n",
    "    sync_url  = ORD_BASE.rstrip(\"/\") + \"/sync\"\n",
    "\n",
    "    base_dir  = BRONZE_ROOT / f\"release_date={sync_date}\" / \"source=ord\" / \"release_type=api_sync\" / \"dataset=all\"\n",
    "    chunk_dir = base_dir / \"chunks\"\n",
    "    ensure_dir(chunk_dir)\n",
    "\n",
    "    data = ord_get_json(sync_url, {\"LastChangeDate\": since})\n",
    "    write_json(chunk_dir / f\"sync_list_since_{since}.json\", data)\n",
    "\n",
    "    changed = data.get(\"Organisations\", []) or []\n",
    "    bar = tqdm(total=len(changed), unit=\"org\", desc=\"SYNC\", leave=True, **TQDM_KW)\n",
    "    for o in changed:\n",
    "        link = o.get(\"OrgLink\")\n",
    "        if not link:\n",
    "            bar.update(1); \n",
    "            continue\n",
    "        full = ord_get_full_org(link)\n",
    "        oid  = full.get(\"OrgId\") or urlparse(link).path.rstrip(\"/\").split(\"/\")[-1].split(\"?\",1)[0]\n",
    "        write_json(chunk_dir / f\"org_{oid}.json\", full)\n",
    "        bar.update(1)\n",
    "    bar.close()\n",
    "\n",
    "    write_json(base_dir / \"_manifest.json\", {\n",
    "        \"api\": ORD_BASE, \"release_date\": sync_date, \"release_type\": \"api_sync\",\n",
    "        \"downloaded_at_utc\": now_utc_iso(), \"params\":{\"LastChangeDate\": since},\n",
    "        \"changed_count\": len(changed)\n",
    "    })\n",
    "    set_wm(\"ord_last_change_date\", datetime.now(timezone.utc).strftime(\"%Y-%m-%d\"))\n",
    "    print(f\"[OK] Sync complete → {base_dir} (changed orgs: {len(changed)})\")\n",
    "    return base_dir\n",
    "\n",
    "# ------------------- FLATTENER: tidy tables -------------------\n",
    "def scalar(x):\n",
    "    if x is None or isinstance(x, (str, int, float, bool)):\n",
    "        return x\n",
    "    if isinstance(x, dict):\n",
    "        for k in (\"extension\",\"value\",\"_\",\"text\",\"#text\",\"displayName\",\"code\",\"id\"):\n",
    "            if k in x and isinstance(x[k], (str, int, float, bool)):\n",
    "                return x[k]\n",
    "        if len(x)==1:\n",
    "            v = next(iter(x.values()))\n",
    "            if isinstance(v,(str,int,float,bool)): return v\n",
    "    if isinstance(x, list):\n",
    "        for it in x:\n",
    "            s = scalar(it)\n",
    "            if s is not None: return s\n",
    "    try:\n",
    "        return json.dumps(x, ensure_ascii=False, separators=(\",\",\":\"))\n",
    "    except Exception:\n",
    "        return str(x)\n",
    "\n",
    "def pick_org(j):\n",
    "    if isinstance(j, dict) and \"Organisation\" in j and isinstance(j[\"Organisation\"], dict):\n",
    "        return j[\"Organisation\"]\n",
    "    if isinstance(j, dict):\n",
    "        return j\n",
    "    return {}\n",
    "\n",
    "def get_org_id(org):\n",
    "    raw = org.get(\"OrgId\")\n",
    "    if isinstance(raw, dict) and \"extension\" in raw:\n",
    "        return str(raw[\"extension\"])\n",
    "    return str(scalar(raw) or \"\")\n",
    "\n",
    "def get_record_class(org):\n",
    "    rc = org.get(\"orgRecordClass\") or org.get(\"OrgRecordClass\")\n",
    "    rc = str(scalar(rc) or \"\")\n",
    "    label = {\"RC1\":\"HSCOrg\", \"RC2\":\"HSCSite\"}.get(rc.upper(), \"\")\n",
    "    return rc, label\n",
    "\n",
    "def ensure_str(x): return \"\" if x is None else str(x)\n",
    "\n",
    "def deep_find_first_key(obj, key_regex):\n",
    "    pat = re.compile(key_regex, re.IGNORECASE)\n",
    "    stack = [obj]; seen=set()\n",
    "    while stack:\n",
    "        cur = stack.pop()\n",
    "        if id(cur) in seen: continue\n",
    "        seen.add(id(cur))\n",
    "        if isinstance(cur, dict):\n",
    "            for k,v in cur.items():\n",
    "                if isinstance(k, str) and pat.search(k):\n",
    "                    val = scalar(v)\n",
    "                    if val not in (None,\"\",\"null\"): return val\n",
    "            for v in cur.values():\n",
    "                if isinstance(v,(dict,list)): stack.append(v)\n",
    "        elif isinstance(cur, list):\n",
    "            for it in cur:\n",
    "                if isinstance(it,(dict,list)): stack.append(it)\n",
    "    return None\n",
    "\n",
    "def clean_postcode(pc):\n",
    "    if not pc: return \"\", \"\"\n",
    "    s = re.sub(r\"\\s+\",\"\", str(pc)).upper()\n",
    "    spaced = s[:-3] + \" \" + s[-3:] if len(s)>3 else s\n",
    "    return s, spaced\n",
    "\n",
    "def extract_address_fields(org):\n",
    "    candidates=[]\n",
    "    for path in [(\"GeoLoc\",\"Location\"), (\"PostalAddress\",), (\"Address\",), (\"Contact\",\"Address\")]:\n",
    "        cur = org; ok=True\n",
    "        for k in path:\n",
    "            if isinstance(cur, dict) and k in cur:\n",
    "                cur = cur[k]\n",
    "            else:\n",
    "                ok=False; break\n",
    "        if ok and isinstance(cur, dict):\n",
    "            candidates.append(cur)\n",
    "    addr={}\n",
    "    for cand in candidates:\n",
    "        keys = {k.lower() for k in cand.keys()}\n",
    "        if keys & {\"addrln1\",\"addrln2\",\"addrln3\",\"addrln4\",\"addrl1\",\"addrl2\",\"town\",\"city\",\"county\",\"postcode\"}:\n",
    "            addr=cand; break\n",
    "        if any(k.lower()==\"postcode\" for k in cand.keys()):\n",
    "            addr=cand; break\n",
    "    def pick(*names):\n",
    "        for n in names:\n",
    "            if isinstance(addr, dict) and n in addr:\n",
    "                return scalar(addr[n])\n",
    "        return None\n",
    "    line1 = pick(\"AddrLn1\",\"Addrl1\",\"Address1\")\n",
    "    line2 = pick(\"AddrLn2\",\"Addrl2\",\"Address2\")\n",
    "    line3 = pick(\"AddrLn3\",\"Addrl3\",\"Address3\")\n",
    "    line4 = pick(\"AddrLn4\",\"Addrl4\",\"Address4\")\n",
    "    city  = pick(\"Town\",\"City\",\"Locality\")\n",
    "    county= pick(\"County\")\n",
    "    country = pick(\"Country\")\n",
    "    pc = pick(\"PostCode\",\"Postcode\",\"Post_Code\",\"Post Code\")\n",
    "    if not pc:\n",
    "        pc = deep_find_first_key(org, r\"post\\s*code|postcode\")\n",
    "    parts = [line1,line2,line3,line4,city,county,country]\n",
    "    addr_full = \", \".join([ensure_str(x).strip() for x in parts if x and str(x).strip()!=\"\"])\n",
    "    pc_compact, pc_spaced = clean_postcode(pc)\n",
    "    return {\n",
    "        \"AddrLine1\": ensure_str(line1),\n",
    "        \"AddrLine2\": ensure_str(line2),\n",
    "        \"AddrLine3\": ensure_str(line3),\n",
    "        \"AddrLine4\": ensure_str(line4),\n",
    "        \"TownCity\": ensure_str(city),\n",
    "        \"County\": ensure_str(county),\n",
    "        \"Country\": ensure_str(country),\n",
    "        \"PostCode\": pc_compact,\n",
    "        \"PostCodeSpaced\": pc_spaced,\n",
    "        \"AddressFull\": addr_full\n",
    "    }\n",
    "\n",
    "def extract_dates(org):\n",
    "    out=[]; d = org.get(\"Date\")\n",
    "    if isinstance(d, list):\n",
    "        for item in d:\n",
    "            if isinstance(item, dict):\n",
    "                out.append({\"DateType\": ensure_str(item.get(\"Type\")),\n",
    "                            \"Start\": ensure_str(item.get(\"Start\")),\n",
    "                            \"End\": ensure_str(item.get(\"End\"))})\n",
    "    return out\n",
    "\n",
    "def extract_roles(org):\n",
    "    out=[]; R = org.get(\"Roles\")\n",
    "    def emit(it):\n",
    "        if not isinstance(it, dict): return\n",
    "        rid = scalar(it.get(\"id\") or it.get(\"idCode\") or it.get(\"Id\") or it.get(\"code\"))\n",
    "        primary = bool(it.get(\"primaryRole\", False))\n",
    "        rstat = ensure_str(scalar(it.get(\"Status\")))\n",
    "        dates=[]; d = it.get(\"Date\")\n",
    "        if isinstance(d, list):\n",
    "            for di in d:\n",
    "                if isinstance(di, dict):\n",
    "                    dates.append({\"Type\": ensure_str(di.get(\"Type\")),\n",
    "                                  \"Start\": ensure_str(di.get(\"Start\")),\n",
    "                                  \"End\": ensure_str(di.get(\"End\"))})\n",
    "        out.append({\"RoleId\": ensure_str(rid), \"PrimaryRole\": primary,\n",
    "                    \"RoleStatus\": rstat, \"RoleDates\": dates})\n",
    "    if isinstance(R, list):\n",
    "        for it in R: emit(it)\n",
    "    elif isinstance(R, dict):\n",
    "        rl = R.get(\"Role\")\n",
    "        if isinstance(rl, list):\n",
    "            for it in rl: emit(it)\n",
    "        elif isinstance(rl, dict):\n",
    "            emit(rl)\n",
    "    return out\n",
    "\n",
    "def extract_rels(org):\n",
    "    out=[]; R = org.get(\"Rels\")\n",
    "    if not isinstance(R, dict): return out\n",
    "    rl = R.get(\"Rel\")\n",
    "    items = rl if isinstance(rl, list) else ([rl] if isinstance(rl, dict) else [])\n",
    "    for it in items:\n",
    "        if not isinstance(it, dict): continue\n",
    "        rel_id = ensure_str(scalar(it.get(\"id\")))\n",
    "        rstat  = ensure_str(scalar(it.get(\"Status\")))\n",
    "        target = it.get(\"Target\") or {}\n",
    "        tgt_org_raw = target.get(\"OrgId\")\n",
    "        if isinstance(tgt_org_raw, dict) and \"extension\" in tgt_org_raw:\n",
    "            tgt_org = ensure_str(tgt_org_raw[\"extension\"])\n",
    "        else:\n",
    "            tgt_org = ensure_str(scalar(tgt_org_raw))\n",
    "        tgt_role_raw = target.get(\"PrimaryRoleId\") or {}\n",
    "        tgt_role = ensure_str(scalar(tgt_role_raw.get(\"id\") if isinstance(tgt_role_raw, dict) else tgt_role_raw))\n",
    "        dates=[]; d = it.get(\"Date\")\n",
    "        if isinstance(d, list):\n",
    "            for di in d:\n",
    "                if isinstance(di, dict):\n",
    "                    dates.append({\"Type\": ensure_str(di.get(\"Type\")),\n",
    "                                  \"Start\": ensure_str(di.get(\"Start\")),\n",
    "                                  \"End\": ensure_str(di.get(\"End\"))})\n",
    "        out.append({\"RelId\": rel_id, \"RelStatus\": rstat,\n",
    "                    \"TargetOrgId\": tgt_org, \"TargetPrimaryRoleId\": tgt_role,\n",
    "                    \"RelDates\": dates})\n",
    "    return out\n",
    "\n",
    "def extract_succs(org):\n",
    "    out=[]; S = org.get(\"Succs\")\n",
    "    if not isinstance(S, dict): return out\n",
    "    sc = S.get(\"Succ\")\n",
    "    items = sc if isinstance(sc, list) else ([sc] if isinstance(sc, dict) else [])\n",
    "    for it in items:\n",
    "        if not isinstance(it, dict): continue\n",
    "        typ = ensure_str(scalar(it.get(\"Type\")))\n",
    "        target = it.get(\"Target\") or {}\n",
    "        tgt_org_raw = target.get(\"OrgId\")\n",
    "        if isinstance(tgt_org_raw, dict) and \"extension\" in tgt_org_raw:\n",
    "            tgt_org = ensure_str(tgt_org_raw[\"extension\"])\n",
    "        else:\n",
    "            tgt_org = ensure_str(scalar(tgt_org_raw))\n",
    "        tgt_role_raw = target.get(\"PrimaryRoleId\") or {}\n",
    "        tgt_role = ensure_str(scalar(tgt_role_raw.get(\"id\") if isinstance(tgt_role_raw, dict) else tgt_role_raw))\n",
    "        dates=[]; d = it.get(\"Date\")\n",
    "        if isinstance(d, list):\n",
    "            for di in d:\n",
    "                if isinstance(di, dict):\n",
    "                    dates.append({\"Type\": ensure_str(di.get(\"Type\")),\n",
    "                                  \"Start\": ensure_str(di.get(\"Start\")),\n",
    "                                  \"End\": ensure_str(di.get(\"End\"))})\n",
    "        out.append({\"SuccType\": typ, \"TargetOrgId\": tgt_org,\n",
    "                    \"TargetPrimaryRoleId\": tgt_role, \"SuccDates\": dates})\n",
    "    return out\n",
    "\n",
    "def make_clean_extracts():\n",
    "    # locate newest baseline chunk dir\n",
    "    chunks_dirs = sorted(BRONZE_ROOT.glob(\"release_date=*/source=ord/release_type=api_baseline/dataset=*/chunks\"))\n",
    "    assert chunks_dirs, \"No baseline chunks folder found.\"\n",
    "    latest_chunks = chunks_dirs[-1]\n",
    "\n",
    "    org_rows, date_rows, role_rows, rel_rows, succ_rows = [], [], [], [], []\n",
    "    files = list(latest_chunks.glob(\"org_*.json\"))\n",
    "\n",
    "    for fp in files:\n",
    "        j = read_json(fp, {})\n",
    "        org = pick_org(j)\n",
    "        oid = get_org_id(org)\n",
    "        name = ensure_str(scalar(org.get(\"Name\")))\n",
    "        status = ensure_str(scalar(org.get(\"Status\")))\n",
    "        lcd = ensure_str(scalar(org.get(\"LastChangeDate\")))\n",
    "        rc_code, rc_label = get_record_class(org)\n",
    "        addr = extract_address_fields(org)\n",
    "\n",
    "        org_rows.append({\n",
    "            \"OrgId\": oid,\n",
    "            \"Name\": name,\n",
    "            \"Status\": status,\n",
    "            \"IsActive\": (status.strip().lower()==\"active\"),\n",
    "            \"OrgRecordClass\": rc_code,\n",
    "            \"OrgRecordClassLabel\": rc_label,\n",
    "            \"LastChangeDate\": lcd,\n",
    "            **addr\n",
    "        })\n",
    "\n",
    "        for d in extract_dates(org):\n",
    "            date_rows.append({\"OrgId\": oid, **d})\n",
    "\n",
    "        for r in extract_roles(org):\n",
    "            if r[\"RoleDates\"]:\n",
    "                for rd in r[\"RoleDates\"]:\n",
    "                    role_rows.append({\"OrgId\": oid,\n",
    "                                      \"RoleId\": r[\"RoleId\"],\n",
    "                                      \"PrimaryRole\": r[\"PrimaryRole\"],\n",
    "                                      \"RoleStatus\": r[\"RoleStatus\"],\n",
    "                                      \"RoleDateType\": rd[\"Type\"],\n",
    "                                      \"RoleStart\": rd[\"Start\"],\n",
    "                                      \"RoleEnd\": rd[\"End\"]})\n",
    "            else:\n",
    "                role_rows.append({\"OrgId\": oid,\n",
    "                                  \"RoleId\": r[\"RoleId\"],\n",
    "                                  \"PrimaryRole\": r[\"PrimaryRole\"],\n",
    "                                  \"RoleStatus\": r[\"RoleStatus\"],\n",
    "                                  \"RoleDateType\": \"\",\n",
    "                                  \"RoleStart\": \"\",\n",
    "                                  \"RoleEnd\": \"\"})\n",
    "\n",
    "        for rel in extract_rels(org):\n",
    "            if rel[\"RelDates\"]:\n",
    "                for rd in rel[\"RelDates\"]:\n",
    "                    rel_rows.append({\"OrgId\": oid,\n",
    "                                     \"RelId\": rel[\"RelId\"],\n",
    "                                     \"RelStatus\": rel[\"RelStatus\"],\n",
    "                                     \"TargetOrgId\": rel[\"TargetOrgId\"],\n",
    "                                     \"TargetPrimaryRoleId\": rel[\"TargetPrimaryRoleId\"],\n",
    "                                     \"RelDateType\": rd[\"Type\"],\n",
    "                                     \"RelStart\": rd[\"Start\"],\n",
    "                                     \"RelEnd\": rd[\"End\"]})\n",
    "            else:\n",
    "                rel_rows.append({\"OrgId\": oid,\n",
    "                                 \"RelId\": rel[\"RelId\"],\n",
    "                                 \"RelStatus\": rel[\"RelStatus\"],\n",
    "                                 \"TargetOrgId\": rel[\"TargetOrgId\"],\n",
    "                                 \"TargetPrimaryRoleId\": rel[\"TargetPrimaryRoleId\"],\n",
    "                                 \"RelDateType\": \"\",\n",
    "                                 \"RelStart\": \"\",\n",
    "                                 \"RelEnd\": \"\"})\n",
    "\n",
    "        for sc in extract_succs(org):\n",
    "            if sc[\"SuccDates\"]:\n",
    "                for sd in sc[\"SuccDates\"]:\n",
    "                    succ_rows.append({\"OrgId\": oid,\n",
    "                                      \"SuccType\": sc[\"SuccType\"],\n",
    "                                      \"TargetOrgId\": sc[\"TargetOrgId\"],\n",
    "                                      \"TargetPrimaryRoleId\": sc[\"TargetPrimaryRoleId\"],\n",
    "                                      \"SuccDateType\": sd[\"Type\"],\n",
    "                                      \"SuccStart\": sd[\"Start\"],\n",
    "                                      \"SuccEnd\": sd[\"End\"]})\n",
    "            else:\n",
    "                succ_rows.append({\"OrgId\": oid,\n",
    "                                  \"SuccType\": sc[\"SuccType\"],\n",
    "                                  \"TargetOrgId\": sc[\"TargetOrgId\"],\n",
    "                                  \"TargetPrimaryRoleId\": sc[\"TargetPrimaryRoleId\"],\n",
    "                                  \"SuccDateType\": \"\",\n",
    "                                  \"SuccStart\": \"\",\n",
    "                                  \"SuccEnd\": \"\"})\n",
    "\n",
    "    orgs_df  = pd.DataFrame(org_rows).drop_duplicates(subset=[\"OrgId\"]).sort_values(\"OrgId\").reset_index(drop=True)\n",
    "    dates_df = pd.DataFrame(date_rows).sort_values([\"OrgId\",\"DateType\",\"Start\"]).reset_index(drop=True)\n",
    "    roles_df = pd.DataFrame(role_rows).sort_values([\"OrgId\",\"RoleId\",\"RoleStart\"]).reset_index(drop=True)\n",
    "    rels_df  = pd.DataFrame(rel_rows).sort_values([\"OrgId\",\"RelId\",\"RelStart\"]).reset_index(drop=True)\n",
    "    succ_df  = pd.DataFrame(succ_rows).sort_values([\"OrgId\",\"SuccType\",\"SuccStart\"]).reset_index(drop=True)\n",
    "\n",
    "    extracts = BRONZE_ROOT / \"extracts\"\n",
    "    ensure_dir(extracts)\n",
    "    ts = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    paths = {\n",
    "        \"orgs_csv\":  extracts / f\"orgs_{ts}.csv\",\n",
    "        \"orgs_parq\": extracts / f\"orgs_{ts}.parquet\",\n",
    "        \"dates_csv\": extracts / f\"org_dates_{ts}.csv\",\n",
    "        \"roles_csv\": extracts / f\"org_roles_{ts}.csv\",\n",
    "        \"rels_csv\":  extracts / f\"org_rels_{ts}.csv\",\n",
    "        \"succ_csv\":  extracts / f\"org_succs_{ts}.csv\",\n",
    "    }\n",
    "    orgs_df.to_csv(paths[\"orgs_csv\"], index=False)\n",
    "    orgs_df.to_parquet(paths[\"orgs_parq\"], index=False)\n",
    "    dates_df.to_csv(paths[\"dates_csv\"], index=False)\n",
    "    roles_df.to_csv(paths[\"roles_csv\"], index=False)\n",
    "    rels_df.to_csv(paths[\"rels_csv\"], index=False)\n",
    "    succ_df.to_csv(paths[\"succ_csv\"], index=False)\n",
    "\n",
    "    print(\"Wrote tidy extracts:\")\n",
    "    for k,p in paths.items(): print(f\"- {k}: {p}\")\n",
    "    display(orgs_df.head(10))\n",
    "    return orgs_df, roles_df, rels_df, succ_df\n",
    "\n",
    "# ------------------- RUN -------------------\n",
    "ensure_dir(BRONZE_ROOT)\n",
    "print(\"Bronze root:\", BRONZE_ROOT.resolve())\n",
    "\n",
    "# If ROLE_IDS is None, auto-discover ALL role IDs from /roles (no 406 risk)\n",
    "if ROLE_IDS is None:\n",
    "    print(\"Discovering all Role IDs from /roles ...\")\n",
    "    ROLE_IDS = discover_role_ids()\n",
    "    print(f\"Discovered {len(ROLE_IDS)} roles\")\n",
    "\n",
    "# Baseline with minimal single-line progress, checkpoints, dedupe\n",
    "baseline_dir = baseline_roles_with_progress(ROLE_IDS)\n",
    "\n",
    "# # Optional: create tidy extracts\n",
    "# if MAKE_CLEAN_EXTRACTS:\n",
    "#     orgs_df, roles_df, rels_df, succ_df = make_clean_extracts()\n",
    "\n",
    "# Later: run deltas only (uses same minimal bar)\n",
    "# sync_dir = incremental_sync_with_progress()\n",
    "# orgs_df, roles_df, rels_df, succ_df = make_clean_extracts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d52050c-9173-4b2b-9b2b-bcc9068d976c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

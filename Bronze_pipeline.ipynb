{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47a5a65d-2a8d-4e08-a72e-f83bc9367009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bronze root: C:\\Users\\NikhilYadav\\Desktop\\NHS ODS\\bronze\\ods\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALL ROLES:   0%|          | 0/15613 [00:00<?, ?org/s]\n",
      "\u001b[Ae RO177:   0%|          | 0/15270 [00:00<?, ?org/s]\n",
      "ALL ROLES:   6%|▋         | 1000/15613 [04:56<1:12:15,  3.37org/s]]\n",
      "ALL ROLES:  13%|█▎        | 2000/15613 [10:19<1:10:45,  3.21org/s]]\n",
      "ALL ROLES:  19%|█▉        | 3000/15613 [15:49<1:07:20,  3.12org/s]]\n",
      "ALL ROLES:  26%|██▌       | 4000/15613 [21:15<1:02:25,  3.10org/s]]\n",
      "ALL ROLES:  32%|███▏      | 5000/15613 [26:34<56:47,  3.11org/s]   \n",
      "ALL ROLES:  38%|███▊      | 6000/15613 [31:56<51:31,  3.11org/s]]\n",
      "ALL ROLES:  45%|████▍     | 7000/15613 [37:16<46:06,  3.11org/s]]\n",
      "ALL ROLES:  51%|█████     | 8000/15613 [42:23<40:10,  3.16org/s]]\n",
      "ALL ROLES:  58%|█████▊    | 9000/15613 [48:20<36:15,  3.04org/s]]\n",
      "ALL ROLES:  64%|██████▍   | 10000/15613 [53:17<29:52,  3.13org/s]]\n",
      "ALL ROLES:  70%|███████   | 11000/15613 [58:14<24:01,  3.20org/s]]\n",
      "ALL ROLES:  77%|███████▋  | 12000/15613 [1:03:13<18:33,  3.24org/s]]\n",
      "ALL ROLES:  83%|████████▎ | 13000/15613 [1:08:07<13:14,  3.29org/s]]\n",
      "ALL ROLES:  90%|████████▉ | 14000/15613 [1:12:59<08:04,  3.33org/s]]\n",
      "ALL ROLES:  96%|█████████▌| 15000/15613 [1:17:51<03:02,  3.36org/s]]\n",
      "ALL ROLES:  98%|█████████▊| 15269/15613 [1:19:09<01:42,  3.37org/s]]\n",
      "\u001b[A                                                                 \n",
      "\u001b[Ae RO98:   0%|          | 0/343 [00:00<?, ?org/s]\n",
      "ALL ROLES: 100%|█████████▉| 15611/15613 [1:20:49<00:00,  3.37org/s]\n",
      "ALL ROLES: 100%|█████████▉| 15611/15613 [1:20:49<00:00,  3.22org/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Baseline complete → C:\\Users\\NikhilYadav\\Desktop\\NHS ODS\\bronze\\ods\\release_date=2025-09-22\\source=ord\\release_type=api_baseline\\dataset=roles\n",
      "Org files: 15611\n",
      "Flattened CSV: C:\\Users\\NikhilYadav\\Desktop\\NHS ODS\\bronze\\ods\\extracts\\snapshot_2025-09-22.csv\n",
      "Flattened Parquet: C:\\Users\\NikhilYadav\\Desktop\\NHS ODS\\bronze\\ods\\extracts\\snapshot_2025-09-22.parquet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OrgId</th>\n",
       "      <th>Name</th>\n",
       "      <th>Status</th>\n",
       "      <th>OrgRecordClass</th>\n",
       "      <th>PostCode</th>\n",
       "      <th>LastChangeDate</th>\n",
       "      <th>PrimaryRoles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  OrgId  Name Status OrgRecordClass PostCode LastChangeDate PrimaryRoles\n",
       "0  None  None   None           None     None           None             "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === NHS ODS Bronze (Jupyter) — baseline + sync WITH PROGRESS BARS ===\n",
    "# - Spec-compliant params (_format=json), 1-based Offset\n",
    "# - Overall + per-role tqdm progress (percent, time, ETA)\n",
    "# - Immutable Bronze, manifests, watermarks\n",
    "# - Optional flatten to CSV/Parquet\n",
    "\n",
    "import os, json, time, hashlib, math, requests\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Optional, Tuple\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ------------------- USER SETTINGS -------------------\n",
    "BRONZE_ROOT = Path(r\"C:\\Users\\NikhilYadav\\Desktop\\NHS ODS\\bronze\\ods\")\n",
    "ORD_BASE    = \"https://directory.spineservices.nhs.uk/ORD/2-0-0\"\n",
    "RATE_LIMIT_RPS = 4\n",
    "\n",
    "# Roles to baseline first (tweak as you like)\n",
    "ROLE_IDS: List[str] = [\"RO177\", \"RO98\"]  # RO177 = Prescribing Cost Centre; RO98 = CCG (legacy)\n",
    "USE_ROLES_PARAM_IF_NEEDED = True         # fallback to Roles= if PrimaryRoleId is rejected\n",
    "PAGE_LIMIT = 1000                        # page size\n",
    "\n",
    "MAKE_FLATTEN_EXTRACT = True\n",
    "\n",
    "# ------------------- HELPERS -------------------\n",
    "def now_utc_iso() -> str:\n",
    "    return datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "def ensure_dir(p: Path) -> None:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def write_json(path: Path, obj: Any) -> None:\n",
    "    ensure_dir(path.parent)\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "def read_json(path: Path, default=None):\n",
    "    if not path.exists():\n",
    "        return default\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def sha256_file(path: Path) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with path.open(\"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(1024 * 1024), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "# watermarks\n",
    "def wm_path() -> Path: return BRONZE_ROOT / \"_watermarks.json\"\n",
    "def get_wm() -> Dict[str, Any]: return read_json(wm_path(), default={}) or {}\n",
    "def set_wm(key: str, val: Any) -> None:\n",
    "    wm = get_wm(); wm[key] = val; write_json(wm_path(), wm)\n",
    "\n",
    "# ------------------- ORD HTTP (spec-compliant) -------------------\n",
    "session = requests.Session()\n",
    "session.headers.update({\"User-Agent\": \"ods-bronze/1.3\"})  # friendly UA only\n",
    "\n",
    "def rate_sleep():\n",
    "    time.sleep(1.0 / max(RATE_LIMIT_RPS, 1))\n",
    "\n",
    "def ord_request(url: str, params: Dict[str, Any]) -> requests.Response:\n",
    "    \"\"\"\n",
    "    Low-level GET returning the raw Response (so we can read headers like X-Total-Count).\n",
    "    - adds _format=json (lowercase)\n",
    "    - uses exact param names per spec\n",
    "    \"\"\"\n",
    "    q = dict(params)\n",
    "    q[\"_format\"] = \"json\"\n",
    "    rate_sleep()\n",
    "    r = session.get(url, params=q, timeout=60, allow_redirects=True)\n",
    "    return r\n",
    "\n",
    "def ord_get_json(url: str, params: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    r = ord_request(url, params)\n",
    "    if not (200 <= r.status_code < 300):\n",
    "        raise RuntimeError(f\"ORD GET failed {r.status_code}. URL: {r.url}\\nBody: {r.text}\")\n",
    "    return r.json()\n",
    "\n",
    "def ord_get_full_org(link: str) -> Dict[str, Any]:\n",
    "    if \"_format=\" not in link:\n",
    "        sep = \"&\" if \"?\" in link else \"?\"\n",
    "        link = f\"{link}{sep}_format=json\"\n",
    "    rate_sleep()\n",
    "    r = session.get(link, timeout=60, allow_redirects=True)\n",
    "    if not (200 <= r.status_code < 300):\n",
    "        raise RuntimeError(f\"ORD org GET failed {r.status_code}. URL: {link}\\nBody: {r.text}\")\n",
    "    return r.json()\n",
    "\n",
    "# ------------------- TOTAL COUNT DISCOVERY -------------------\n",
    "def get_total_for_role(search_url: str, role_id: str, use_roles_param: bool) -> Tuple[Optional[int], Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Try to fetch X-Total-Count for a role using a tiny page (Limit=1, Offset=1).\n",
    "    Returns (total_or_None, params_used).\n",
    "    Some deployments don’t set X-Total-Count — we'll handle that.\n",
    "    \"\"\"\n",
    "    # Try PrimaryRoleId first\n",
    "    params = {\"PrimaryRoleId\": role_id, \"Limit\": 1, \"Offset\": 1}\n",
    "    r = ord_request(search_url, params)\n",
    "    if 200 <= r.status_code < 300:\n",
    "        total = r.headers.get(\"X-Total-Count\")\n",
    "        if total and total.isdigit():\n",
    "            return int(total), {\"PrimaryRoleId\": role_id}\n",
    "        else:\n",
    "            return None, {\"PrimaryRoleId\": role_id}\n",
    "    # If rejected and fallback allowed, try Roles=\n",
    "    if use_roles_param:\n",
    "        params = {\"Roles\": role_id, \"Limit\": 1, \"Offset\": 1}\n",
    "        r = ord_request(search_url, params)\n",
    "        if 200 <= r.status_code < 300:\n",
    "            total = r.headers.get(\"X-Total-Count\")\n",
    "            if total and total.isdigit():\n",
    "                return int(total), {\"Roles\": role_id}\n",
    "            else:\n",
    "                return None, {\"Roles\": role_id}\n",
    "        else:\n",
    "            raise RuntimeError(f\"Unable to get total for {role_id}. URL: {r.url}\\nBody: {r.text}\")\n",
    "    else:\n",
    "        raise RuntimeError(f\"Unable to get total for {role_id}. URL: {r.url}\\nBody: {r.text}\")\n",
    "\n",
    "# ------------------- BASELINE (with progress) -------------------\n",
    "def baseline_roles_with_progress(role_ids: List[str]) -> Path:\n",
    "    ensure_dir(BRONZE_ROOT)\n",
    "    release_date = datetime.now(timezone.utc).strftime(\"%Y-%m-%d\")\n",
    "    search_url   = ORD_BASE.rstrip(\"/\") + \"/organisations\"\n",
    "\n",
    "    base_dir  = BRONZE_ROOT / f\"release_date={release_date}\" / \"source=ord\" / \"release_type=api_baseline\" / \"dataset=roles\"\n",
    "    chunk_dir = base_dir / \"chunks\"\n",
    "    ensure_dir(chunk_dir)\n",
    "\n",
    "    # 1) discover totals per role (so we can show overall + per-role %)\n",
    "    role_plans = []\n",
    "    grand_total = 0\n",
    "    totals_known = True\n",
    "    for rid in role_ids:\n",
    "        total, base_params = get_total_for_role(search_url, rid, USE_ROLES_PARAM_IF_NEEDED)\n",
    "        role_plans.append({\"role\": rid, \"total\": total, \"base_params\": base_params})\n",
    "        if total is None:\n",
    "            totals_known = False\n",
    "        else:\n",
    "            grand_total += total\n",
    "\n",
    "    # 2) manifests\n",
    "    manifest = {\n",
    "        \"api\": ORD_BASE, \"release_date\": release_date, \"release_type\": \"api_baseline\",\n",
    "        \"downloaded_at_utc\": now_utc_iso(), \"roles\": role_ids,\n",
    "        \"role_plans\": role_plans,\n",
    "        \"summary_chunks\": [], \"org_records\": []\n",
    "    }\n",
    "\n",
    "    # 3) Overall progress bar (if we know totals)\n",
    "    overall = tqdm(total=grand_total if totals_known else None, unit=\"org\", desc=\"ALL ROLES\", leave=True)\n",
    "\n",
    "    # 4) crawl each role with a per-role progress bar\n",
    "    def page_once(params: Dict[str, Any], offset: int, limit: int) -> Dict[str, Any]:\n",
    "        p = dict(params)\n",
    "        # 1-based Offset\n",
    "        p[\"Limit\"]  = limit\n",
    "        p[\"Offset\"] = max(1, offset)\n",
    "        return ord_get_json(search_url, p)\n",
    "\n",
    "    for plan in role_plans:\n",
    "        rid = plan[\"role\"]\n",
    "        total = plan[\"total\"]\n",
    "        base_params = plan[\"base_params\"]\n",
    "\n",
    "        per_role = tqdm(total=total if total is not None else None,\n",
    "                        unit=\"org\", desc=f\"Role {rid}\", leave=False)\n",
    "\n",
    "        # 1-based paging\n",
    "        offset = 1\n",
    "        while True:\n",
    "            try:\n",
    "                data = page_once(base_params, offset, PAGE_LIMIT)\n",
    "            except RuntimeError as e:\n",
    "                # last-chance param flip if needed\n",
    "                if \"PrimaryRoleId\" in base_params and USE_ROLES_PARAM_IF_NEEDED:\n",
    "                    base_params = {\"Roles\": rid}\n",
    "                    data = page_once(base_params, offset, PAGE_LIMIT)\n",
    "                else:\n",
    "                    per_role.close()\n",
    "                    overall.close()\n",
    "                    raise\n",
    "\n",
    "            orgs = data.get(\"Organisations\", []) or []\n",
    "            if not orgs:\n",
    "                per_role.close()\n",
    "                break\n",
    "\n",
    "            # save summary\n",
    "            chunk_name = f\"search_{rid}_{offset:09d}.json\"\n",
    "            write_json(chunk_dir / chunk_name, data)\n",
    "            manifest[\"summary_chunks\"].append({\"role\": rid, \"file\": chunk_name, \"count\": len(orgs)})\n",
    "\n",
    "            # fetch each full record\n",
    "            for rec in orgs:\n",
    "                link = rec.get(\"OrgLink\"); oid = rec.get(\"OrgId\")\n",
    "                if not link or not oid:\n",
    "                    continue\n",
    "                try:\n",
    "                    full = ord_get_full_org(link)\n",
    "                    ofile = f\"org_{oid}.json\"\n",
    "                    write_json(chunk_dir / ofile, full)\n",
    "                    manifest[\"org_records\"].append({\"org_id\": oid, \"file\": ofile})\n",
    "                except Exception as ex:\n",
    "                    manifest.setdefault(\"errors\", []).append({\"org\": oid, \"error\": str(ex)})\n",
    "\n",
    "            # progress\n",
    "            per_role.update(len(orgs))\n",
    "            overall.update(len(orgs))\n",
    "\n",
    "            # advance offset by page size (1-based stepping: 1, 1001, 2001, ...)\n",
    "            offset += PAGE_LIMIT\n",
    "\n",
    "            # stop if short page\n",
    "            if len(orgs) < PAGE_LIMIT:\n",
    "                per_role.close()\n",
    "                break\n",
    "\n",
    "    overall.close()\n",
    "    write_json(base_dir / \"_manifest.json\", manifest)\n",
    "\n",
    "    # watermarks\n",
    "    set_wm(\"ord_api_baseline_date\", release_date)\n",
    "    set_wm(\"ord_last_change_date\", (datetime.now(timezone.utc) - timedelta(days=1)).strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "    print(f\"[OK] Baseline complete → {base_dir}\")\n",
    "    print(\"Org files:\", len(manifest[\"org_records\"]))\n",
    "    return base_dir\n",
    "\n",
    "# ------------------- INCREMENTAL SYNC (with progress) -------------------\n",
    "def incremental_sync_with_progress():\n",
    "    wm = get_wm()\n",
    "    since = wm.get(\"ord_last_change_date\")\n",
    "    assert since, \"No watermark found. Run baseline first.\"\n",
    "\n",
    "    sync_date = datetime.now(timezone.utc).strftime(\"%Y-%m-%d\")\n",
    "    sync_url  = ORD_BASE.rstrip(\"/\") + \"/sync\"\n",
    "\n",
    "    base_dir  = BRONZE_ROOT / f\"release_date={sync_date}\" / \"source=ord\" / \"release_type=api_sync\" / \"dataset=all\"\n",
    "    chunk_dir = base_dir / \"chunks\"\n",
    "    ensure_dir(chunk_dir)\n",
    "\n",
    "    data = ord_get_json(sync_url, {\"LastChangeDate\": since})\n",
    "    write_json(chunk_dir / f\"sync_list_since_{since}.json\", data)\n",
    "\n",
    "    changed = data.get(\"Organisations\", []) or []\n",
    "    bar = tqdm(total=len(changed), unit=\"org\", desc=\"SYNC download\", leave=True)\n",
    "    for o in changed:\n",
    "        link = o.get(\"OrgLink\")\n",
    "        if not link: \n",
    "            bar.update(1); \n",
    "            continue\n",
    "        full = ord_get_full_org(link)\n",
    "        oid  = full.get(\"OrgId\") or urlparse(link).path.rstrip(\"/\").split(\"/\")[-1].split(\"?\",1)[0]\n",
    "        write_json(chunk_dir / f\"org_{oid}.json\", full)\n",
    "        bar.update(1)\n",
    "    bar.close()\n",
    "\n",
    "    write_json(base_dir / \"_manifest.json\", {\n",
    "        \"api\": ORD_BASE, \"release_date\": sync_date, \"release_type\": \"api_sync\",\n",
    "        \"downloaded_at_utc\": now_utc_iso(), \"params\":{\"LastChangeDate\": since},\n",
    "        \"changed_count\": len(changed)\n",
    "    })\n",
    "\n",
    "    set_wm(\"ord_last_change_date\", datetime.now(timezone.utc).strftime(\"%Y-%m-%d\"))\n",
    "    print(f\"[OK] Sync complete → {base_dir} (changed orgs: {len(changed)})\")\n",
    "    return base_dir\n",
    "\n",
    "# ------------------- FLATTEN (optional) -------------------\n",
    "def flatten_latest_baseline_to_tabular():\n",
    "    candidates = sorted(BRONZE_ROOT.glob(\"release_date=*/source=ord/release_type=api_baseline/dataset=*/chunks\"))\n",
    "    assert candidates, \"No baseline found.\"\n",
    "    latest_chunks = candidates[-1]\n",
    "\n",
    "    rows=[]\n",
    "    for f in latest_chunks.glob(\"org_*.json\"):\n",
    "        o = read_json(f, {})\n",
    "        rows.append({\n",
    "            \"OrgId\": o.get(\"OrgId\"),\n",
    "            \"Name\": o.get(\"Name\"),\n",
    "            \"Status\": o.get(\"Status\"),\n",
    "            \"OrgRecordClass\": o.get(\"OrgRecordClass\"),\n",
    "            \"PostCode\": o.get(\"PostCode\"),\n",
    "            \"LastChangeDate\": o.get(\"LastChangeDate\"),\n",
    "            \"PrimaryRoles\": \",\".join(sorted({\n",
    "                (r.get(\"id\") or r.get(\"idCode\") or r.get(\"Id\"))\n",
    "                for r in (o.get(\"Roles\") or []) if isinstance(r, dict)\n",
    "            }))\n",
    "        })\n",
    "    df = pd.DataFrame(rows).drop_duplicates(subset=[\"OrgId\"]).sort_values(\"OrgId\")\n",
    "\n",
    "    extracts = BRONZE_ROOT / \"extracts\"\n",
    "    ensure_dir(extracts)\n",
    "    stamp = datetime.now(timezone.utc).strftime(\"%Y-%m-%d\")\n",
    "    csv_path  = extracts / f\"snapshot_{stamp}.csv\"\n",
    "    parq_path = extracts / f\"snapshot_{stamp}.parquet\"\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    df.to_parquet(parq_path, index=False)\n",
    "\n",
    "    print(\"Flattened CSV:\", csv_path)\n",
    "    print(\"Flattened Parquet:\", parq_path)\n",
    "    return df\n",
    "\n",
    "# ------------------- RUN -------------------\n",
    "ensure_dir(BRONZE_ROOT)\n",
    "print(\"Bronze root:\", BRONZE_ROOT.resolve())\n",
    "\n",
    "# BASELINE with progress bars\n",
    "baseline_dir = baseline_roles_with_progress(ROLE_IDS)\n",
    "\n",
    "# LATER: run incremental sync with progress\n",
    "# sync_dir = incremental_sync_with_progress()\n",
    "# _df2 = flatten_latest_baseline_to_tabular(); display(_df2.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e310d5e3-f72f-404d-a359-1b9f8728662d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

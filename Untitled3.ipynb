{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01f6917-6ec5-4a0a-8366-8fef5e9881a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === NHS ODS Bronze (Jupyter) — SMART (baseline-or-sync), fast, no re-downloads ===\n",
    "# - SMART runner: if watermark exists -> /sync (only new/changed); else baseline\n",
    "# - Baseline: dedupe across ALL past runs (won't re-download existing org_*.json)\n",
    "# - Concurrency for full-record GETs + global RPS cap + retries/backoff\n",
    "# - Minimal single-line tqdm bar; per-role bars hidden\n",
    "# - ROLE_IDS=None -> auto-discover all roles from /roles (no 406)\n",
    "# - Clean extracts of orgs/addresses/roles/rels/successors\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "import os, re, json, time, math, threading, random\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Optional, Tuple\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm.auto import tqdm  # notebook-friendly\n",
    "\n",
    "# ------------------- USER SETTINGS -------------------\n",
    "BRONZE_ROOT = Path(r\"C:\\Users\\NikhilYadav\\Desktop\\NHS ODS\\bronze\\ods\")\n",
    "ORD_BASE    = \"https://directory.spineservices.nhs.uk/ORD/2-0-0\"\n",
    "\n",
    "# Speed knobs (tune carefully)\n",
    "RATE_LIMIT_RPS        = 6     # global requests/sec across all threads\n",
    "CONCURRENCY_FULL_ORG  = 16    # concurrent full-record GETs (8–24 is typical sweet spot)\n",
    "PAGE_LIMIT            = 1000  # summary page size\n",
    "\n",
    "# Role selection: provide list OR None to discover all from /roles\n",
    "ROLE_IDS: Optional[List[str]] = None\n",
    "\n",
    "# Fallback to Roles= when PrimaryRoleId is rejected\n",
    "USE_ROLES_PARAM_IF_NEEDED = True\n",
    "\n",
    "# Write tidy CSV/Parquet extracts at the end\n",
    "MAKE_CLEAN_EXTRACTS = True\n",
    "\n",
    "# ------------------- MINIMAL PROGRESS BAR -------------------\n",
    "TQDM_BAR_OVERALL = \"{desc}: {percentage:3.0f}%\"\n",
    "TQDM_KW = dict(bar_format=TQDM_BAR_OVERALL, dynamic_ncols=True, mininterval=1.5, smoothing=0.2, ascii=None)\n",
    "\n",
    "# ------------------- UTILS -------------------\n",
    "def now_utc_iso() -> str:\n",
    "    return datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "def ensure_dir(p: Path) -> None:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def write_json(path: Path, obj: Any) -> None:\n",
    "    ensure_dir(path.parent)\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "def read_json(path: Path, default=None):\n",
    "    if not path.exists():\n",
    "        return default\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# watermarks\n",
    "def wm_path() -> Path: return BRONZE_ROOT / \"_watermarks.json\"\n",
    "def get_wm() -> Dict[str, Any]: return read_json(wm_path(), default={}) or {}\n",
    "def set_wm(key: str, val: Any) -> None:\n",
    "    wm = get_wm(); wm[key] = val; write_json(wm_path(), wm)\n",
    "\n",
    "# checkpoints (resume per role)\n",
    "CKPT_DIR = BRONZE_ROOT / \"_checkpoints\"; ensure_dir(CKPT_DIR)\n",
    "def load_ckpt(role_id: str) -> int:\n",
    "    j = read_json(CKPT_DIR / f\"{role_id}.json\", {})\n",
    "    return int(j.get(\"next_offset\", 1))  # 1-based\n",
    "def save_ckpt(role_id: str, next_offset: int):\n",
    "    write_json(CKPT_DIR / f\"{role_id}.json\", {\"next_offset\": int(next_offset), \"saved_at\": now_utc_iso()})\n",
    "\n",
    "# dedupe set across ALL runs (so re-runs won't re-download)\n",
    "def preload_seen_orgs() -> set:\n",
    "    seen=set()\n",
    "    for p in BRONZE_ROOT.glob(\"release_date=*/source=ord/**/chunks/org_*.json\"):\n",
    "        try:\n",
    "            seen.add(p.stem.replace(\"org_\",\"\"))\n",
    "        except: pass\n",
    "    return seen\n",
    "\n",
    "# ------------------- HTTP SESSION (gzip + retries + pools) -------------------\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    \"User-Agent\": \"ods-bronze/2.0\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate\",\n",
    "})\n",
    "\n",
    "retry = Retry(\n",
    "    total=8,\n",
    "    backoff_factor=0.5,\n",
    "    status_forcelist=(429, 500, 502, 503, 504),\n",
    "    allowed_methods=frozenset([\"GET\"]),\n",
    "    raise_on_status=False,\n",
    ")\n",
    "adapter = HTTPAdapter(max_retries=retry, pool_connections=64, pool_maxsize=64)\n",
    "session.mount(\"https://\", adapter); session.mount(\"http://\", adapter)\n",
    "\n",
    "# global token-bucket-ish RPS limiter (shared across threads)\n",
    "_rate_lock = threading.Lock()\n",
    "_last_tick = [0.0]\n",
    "def rate_sleep():\n",
    "    with _rate_lock:\n",
    "        now = time.time()\n",
    "        min_gap = 1.0 / max(RATE_LIMIT_RPS, 1)\n",
    "        wait = max(0.0, min_gap - (now - _last_tick[0]))\n",
    "        if wait > 0: time.sleep(wait)\n",
    "        _last_tick[0] = time.time()\n",
    "\n",
    "def ord_request(url: str, params: Dict[str, Any]) -> requests.Response:\n",
    "    q = dict(params); q[\"_format\"] = \"json\"  # spec: lowercase\n",
    "    rate_sleep()\n",
    "    return session.get(url, params=q, timeout=60, allow_redirects=True)\n",
    "\n",
    "def ord_get_json(url: str, params: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    r = ord_request(url, params)\n",
    "    if not (200 <= r.status_code < 300):\n",
    "        raise RuntimeError(f\"ORD GET failed {r.status_code}. URL: {r.url}\\nBody: {r.text[:400]}\")\n",
    "    return r.json()\n",
    "\n",
    "def ord_get_full_org(link: str, retries=3, backoff=1.5) -> Dict[str, Any]:\n",
    "    if \"_format=\" not in link:\n",
    "        sep = \"&\" if \"?\" in link else \"?\"\n",
    "        link = f\"{link}{sep}_format=json\"\n",
    "    for attempt in range(1, retries+1):\n",
    "        rate_sleep()\n",
    "        r = session.get(link, timeout=60, allow_redirects=True)\n",
    "        if 200 <= r.status_code < 300:\n",
    "            return r.json()\n",
    "        if attempt == retries:\n",
    "            raise RuntimeError(f\"ORD org GET failed {r.status_code}. URL: {link}\\nBody: {r.text[:400]}\")\n",
    "        time.sleep(backoff**attempt)\n",
    "\n",
    "# ------------------- ROLE DISCOVERY -------------------\n",
    "def discover_role_ids() -> List[str]:\n",
    "    roles_url = f\"{ORD_BASE.rstrip('/')}/roles\"\n",
    "    # JSON first\n",
    "    try:\n",
    "        payload = ord_get_json(roles_url, params={})\n",
    "        ids=set()\n",
    "        def walk(x):\n",
    "            if isinstance(x, dict):\n",
    "                v = x.get(\"id\")\n",
    "                if isinstance(v, str) and v.upper().startswith(\"RO\"):\n",
    "                    ids.add(v.upper())\n",
    "                for vv in x.values(): walk(vv)\n",
    "            elif isinstance(x, list):\n",
    "                for it in x: walk(it)\n",
    "        walk(payload)\n",
    "        if ids: return sorted(ids)\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Fallback XML (tolerant)\n",
    "    r = ord_request(roles_url, params={\"_format\":\"xml\"})\n",
    "    r.raise_for_status()\n",
    "    ids = sorted(set(re.findall(r\">\\s*(RO\\d+)\\s*<\", r.text, flags=re.IGNORECASE)))\n",
    "    return [i.upper() for i in ids]\n",
    "\n",
    "# ------------------- TOTAL COUNT DISCOVERY -------------------\n",
    "def get_total_for_role(search_url: str, role_id: str, use_roles_param: bool) -> Tuple[Optional[int], Dict[str, Any]]:\n",
    "    # Try PrimaryRoleId first\n",
    "    params = {\"PrimaryRoleId\": role_id, \"Limit\": 1, \"Offset\": 1}\n",
    "    r = ord_request(search_url, params)\n",
    "    if 200 <= r.status_code < 300:\n",
    "        total = r.headers.get(\"X-Total-Count\")\n",
    "        return (int(total) if total and total.isdigit() else None, {\"PrimaryRoleId\": role_id})\n",
    "    # Fallback to Roles\n",
    "    if use_roles_param:\n",
    "        params = {\"Roles\": role_id, \"Limit\": 1, \"Offset\": 1}\n",
    "        r = ord_request(search_url, params)\n",
    "        if 200 <= r.status_code < 300:\n",
    "            total = r.headers.get(\"X-Total-Count\")\n",
    "            return (int(total) if total and total.isdigit() else None, {\"Roles\": role_id})\n",
    "        raise RuntimeError(f\"Unable to get total for {role_id}. URL: {r.url}\\nBody: {r.text[:200]}\")\n",
    "    raise RuntimeError(f\"Unable to get total for {role_id}. URL: {r.url}\\nBody: {r.text[:200]}\")\n",
    "\n",
    "# ------------------- BASELINE (fast, deduped) -------------------\n",
    "def baseline_roles_with_progress(role_ids: List[str]) -> Path:\n",
    "    ensure_dir(BRONZE_ROOT)\n",
    "    release_date = datetime.now(timezone.utc).strftime(\"%Y-%m-%d\")\n",
    "    search_url   = ORD_BASE.rstrip(\"/\") + \"/organisations\"\n",
    "\n",
    "    base_dir  = BRONZE_ROOT / f\"release_date={release_date}\" / \"source=ord\" / \"release_type=api_baseline\" / \"dataset=roles\"\n",
    "    chunk_dir = base_dir / \"chunks\"; ensure_dir(chunk_dir)\n",
    "\n",
    "    # plan totals (we'll skip roles with 0 current orgs)\n",
    "    role_plans = []\n",
    "    grand_total = 0; totals_known = True\n",
    "    for rid in role_ids:\n",
    "        total, base_params = get_total_for_role(search_url, rid, USE_ROLES_PARAM_IF_NEEDED)\n",
    "        role_plans.append({\"role\": rid, \"total\": total, \"base_params\": base_params})\n",
    "    role_plans = [p for p in role_plans if (p[\"total\"] or 0) > 0 or p[\"total\"] is None]\n",
    "    for p in role_plans:\n",
    "        if p[\"total\"] is None: totals_known = False\n",
    "        else: grand_total += p[\"total\"]\n",
    "\n",
    "    manifest = {\n",
    "        \"api\": ORD_BASE, \"release_date\": release_date, \"release_type\": \"api_baseline\",\n",
    "        \"downloaded_at_utc\": now_utc_iso(), \"roles\": [p[\"role\"] for p in role_plans],\n",
    "        \"role_plans\": role_plans, \"summary_chunks\": [], \"org_records\": []\n",
    "    }\n",
    "\n",
    "    overall = tqdm(total=grand_total if totals_known else None, unit=\"org\",\n",
    "                   desc=f\"SELECTED ROLES ({len(role_plans)})\", leave=True, **TQDM_KW)\n",
    "\n",
    "    SEEN_ORGS = preload_seen_orgs()\n",
    "\n",
    "    def page_once(params: Dict[str, Any], offset: int, limit: int) -> Dict[str, Any]:\n",
    "        p = dict(params); p[\"Limit\"] = limit; p[\"Offset\"] = max(1, offset)\n",
    "        return ord_get_json(search_url, p)\n",
    "\n",
    "    def fetch_and_write(org_rec):\n",
    "        link = org_rec.get(\"OrgLink\"); oid = org_rec.get(\"OrgId\")\n",
    "        if not link or not oid: return None, False\n",
    "        if oid in SEEN_ORGS:    return oid, False\n",
    "        full = ord_get_full_org(link)\n",
    "        ofile = f\"org_{oid}.json\"; write_json(chunk_dir / ofile, full)\n",
    "        SEEN_ORGS.add(oid)\n",
    "        return oid, True\n",
    "\n",
    "    for plan in role_plans:\n",
    "        rid = plan[\"role\"]; total = plan[\"total\"]; base_params = plan[\"base_params\"]\n",
    "        tqdm.write(f\"→ Crawling {rid} ({total if total is not None else 'unknown'} orgs)\")\n",
    "        offset = load_ckpt(rid)\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                data = page_once(base_params, offset, PAGE_LIMIT)\n",
    "            except RuntimeError:\n",
    "                if \"PrimaryRoleId\" in base_params and USE_ROLES_PARAM_IF_NEEDED:\n",
    "                    base_params = {\"Roles\": rid}\n",
    "                    data = page_once(base_params, offset, PAGE_LIMIT)\n",
    "                else:\n",
    "                    overall.close(); raise\n",
    "\n",
    "            orgs = data.get(\"Organisations\", []) or []\n",
    "            if not orgs:\n",
    "                save_ckpt(rid, 1)\n",
    "                break\n",
    "\n",
    "            # save summary page\n",
    "            chunk_name = f\"search_{rid}_{offset:09d}.json\"\n",
    "            write_json(chunk_dir / chunk_name, data)\n",
    "            manifest[\"summary_chunks\"].append({\"role\": rid, \"file\": chunk_name, \"count\": len(orgs)})\n",
    "\n",
    "            # concurrent full-record fetches (only for unseen orgs)\n",
    "            with ThreadPoolExecutor(max_workers=CONCURRENCY_FULL_ORG) as ex:\n",
    "                futures = [ex.submit(fetch_and_write, rec) for rec in orgs]\n",
    "                for fut in as_completed(futures):\n",
    "                    try:\n",
    "                        fut.result()\n",
    "                    except Exception as e:\n",
    "                        manifest.setdefault(\"errors\", []).append({\"role\": rid, \"error\": str(e)})\n",
    "                    finally:\n",
    "                        overall.update(1)\n",
    "\n",
    "            offset += PAGE_LIMIT; save_ckpt(rid, offset)\n",
    "            if len(orgs) < PAGE_LIMIT:\n",
    "                save_ckpt(rid, 1)\n",
    "                break\n",
    "\n",
    "    write_json(base_dir / \"_manifest.json\", manifest)\n",
    "    set_wm(\"ord_api_baseline_date\", release_date)\n",
    "    set_wm(\"ord_last_change_date\", (datetime.now(timezone.utc) - timedelta(days=1)).strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "    # final count (deduped)\n",
    "    org_files = list((base_dir / \"chunks\").glob(\"org_*.json\"))\n",
    "    print(f\"[OK] Baseline complete → {base_dir}\")\n",
    "    print(\"Org files (newly written in this run):\", len(org_files))\n",
    "    return base_dir\n",
    "\n",
    "# ------------------- INCREMENTAL SYNC (preferred for re-runs) -------------------\n",
    "def incremental_sync_with_progress():\n",
    "    wm = get_wm()\n",
    "    since = wm.get(\"ord_last_change_date\")\n",
    "    assert since, \"No watermark found. Run baseline first.\"\n",
    "\n",
    "    sync_date = datetime.now(timezone.utc).strftime(\"%Y-%m-%d\")\n",
    "    sync_url  = ORD_BASE.rstrip(\"/\") + \"/sync\"\n",
    "\n",
    "    base_dir  = BRONZE_ROOT / f\"release_date={sync_date}\" / \"source=ord\" / \"release_type=api_sync\" / \"dataset=all\"\n",
    "    chunk_dir = base_dir / \"chunks\"; ensure_dir(chunk_dir)\n",
    "\n",
    "    data = ord_get_json(sync_url, {\"LastChangeDate\": since})\n",
    "    write_json(chunk_dir / f\"sync_list_since_{since}.json\", data)\n",
    "\n",
    "    changed = data.get(\"Organisations\", []) or []\n",
    "    bar = tqdm(total=len(changed), unit=\"org\", desc=\"SYNC\", leave=True, **TQDM_KW)\n",
    "\n",
    "    SEEN_ORGS = preload_seen_orgs()\n",
    "    new_count = 0\n",
    "    for o in changed:\n",
    "        link = o.get(\"OrgLink\")\n",
    "        if not link:\n",
    "            bar.update(1); continue\n",
    "        full = ord_get_full_org(link)\n",
    "        oid  = full.get(\"OrgId\") or urlparse(link).path.rstrip(\"/\").split(\"/\")[-1].split(\"?\",1)[0]\n",
    "        if oid in SEEN_ORGS:\n",
    "            # Still write into current run to make this run self-contained for the changed subset\n",
    "            pass\n",
    "        write_json(chunk_dir / f\"org_{oid}.json\", full)\n",
    "        new_count += (0 if oid in SEEN_ORGS else 1)\n",
    "        bar.update(1)\n",
    "    bar.close()\n",
    "\n",
    "    write_json(base_dir / \"_manifest.json\", {\n",
    "        \"api\": ORD_BASE, \"release_date\": sync_date, \"release_type\": \"api_sync\",\n",
    "        \"downloaded_at_utc\": now_utc_iso(), \"params\":{\"LastChangeDate\": since},\n",
    "        \"changed_count\": len(changed), \"new_orgs_first_seen_this_run\": new_count\n",
    "    })\n",
    "    set_wm(\"ord_last_change_date\", datetime.now(timezone.utc).strftime(\"%Y-%m-%d\"))\n",
    "    print(f\"[OK] Sync complete → {base_dir} (changed orgs: {len(changed)}, new orgs: {new_count})\")\n",
    "    return base_dir\n",
    "\n",
    "# ------------------- CLEAN EXTRACTS -------------------\n",
    "def scalar(x):\n",
    "    if x is None or isinstance(x, (str, int, float, bool)): return x\n",
    "    if isinstance(x, dict):\n",
    "        for k in (\"extension\",\"value\",\"_\",\"text\",\"#text\",\"displayName\",\"code\",\"id\"):\n",
    "            if k in x and isinstance(x[k], (str, int, float, bool)): return x[k]\n",
    "        if len(x)==1:\n",
    "            v = next(iter(x.values()))\n",
    "            if isinstance(v,(str,int,float,bool)): return v\n",
    "    if isinstance(x, list):\n",
    "        for it in x:\n",
    "            s = scalar(it)\n",
    "            if s is not None: return s\n",
    "    try: return json.dumps(x, ensure_ascii=False, separators=(\",\",\":\"))\n",
    "    except: return str(x)\n",
    "\n",
    "def pick_org(j):\n",
    "    if isinstance(j, dict) and \"Organisation\" in j and isinstance(j[\"Organisation\"], dict): return j[\"Organisation\"]\n",
    "    if isinstance(j, dict): return j\n",
    "    return {}\n",
    "\n",
    "def get_org_id(org):\n",
    "    raw = org.get(\"OrgId\")\n",
    "    if isinstance(raw, dict) and \"extension\" in raw: return str(raw[\"extension\"])\n",
    "    return str(scalar(raw) or \"\")\n",
    "\n",
    "def get_record_class(org):\n",
    "    rc = org.get(\"orgRecordClass\") or org.get(\"OrgRecordClass\")\n",
    "    rc = str(scalar(rc) or \"\")\n",
    "    label = {\"RC1\":\"HSCOrg\", \"RC2\":\"HSCSite\"}.get(rc.upper(), \"\")\n",
    "    return rc, label\n",
    "\n",
    "def ensure_str(x): return \"\" if x is None else str(x)\n",
    "\n",
    "def deep_find_first_key(obj, key_regex):\n",
    "    pat = re.compile(key_regex, re.IGNORECASE)\n",
    "    stack = [obj]; seen=set()\n",
    "    while stack:\n",
    "        cur = stack.pop()\n",
    "        if id(cur) in seen: continue\n",
    "        seen.add(id(cur))\n",
    "        if isinstance(cur, dict):\n",
    "            for k,v in cur.items():\n",
    "                if isinstance(k, str) and pat.search(k):\n",
    "                    val = scalar(v)\n",
    "                    if val not in (None,\"\",\"null\"): return val\n",
    "            for v in cur.values():\n",
    "                if isinstance(v,(dict,list)): stack.append(v)\n",
    "        elif isinstance(cur, list):\n",
    "            for it in cur:\n",
    "                if isinstance(it,(dict,list)): stack.append(it)\n",
    "    return None\n",
    "\n",
    "def clean_postcode(pc):\n",
    "    if not pc: return \"\", \"\"\n",
    "    s = re.sub(r\"\\s+\",\"\", str(pc)).upper()\n",
    "    spaced = s[:-3] + \" \" + s[-3:] if len(s)>3 else s\n",
    "    return s, spaced\n",
    "\n",
    "def extract_address_fields(org):\n",
    "    candidates=[]\n",
    "    for path in [(\"GeoLoc\",\"Location\"), (\"PostalAddress\",), (\"Address\",), (\"Contact\",\"Address\")]:\n",
    "        cur = org; ok=True\n",
    "        for k in path:\n",
    "            if isinstance(cur, dict) and k in cur: cur = cur[k]\n",
    "            else: ok=False; break\n",
    "        if ok and isinstance(cur, dict): candidates.append(cur)\n",
    "    addr={}\n",
    "    for cand in candidates:\n",
    "        keys = {k.lower() for k in cand.keys()}\n",
    "        if keys & {\"addrln1\",\"addrln2\",\"addrln3\",\"addrln4\",\"addrl1\",\"addrl2\",\"town\",\"city\",\"county\",\"postcode\"}: addr=cand; break\n",
    "        if any(k.lower()==\"postcode\" for k in cand.keys()): addr=cand; break\n",
    "    def pick(*names):\n",
    "        for n in names:\n",
    "            if isinstance(addr, dict) and n in addr: return scalar(addr[n])\n",
    "        return None\n",
    "    line1 = pick(\"AddrLn1\",\"Addrl1\",\"Address1\")\n",
    "    line2 = pick(\"AddrLn2\",\"Addrl2\",\"Address2\")\n",
    "    line3 = pick(\"AddrLn3\",\"Addrl3\",\"Address3\")\n",
    "    line4 = pick(\"AddrLn4\",\"Addrl4\",\"Address4\")\n",
    "    city  = pick(\"Town\",\"City\",\"Locality\")\n",
    "    county= pick(\"County\")\n",
    "    country = pick(\"Country\")\n",
    "    pc = pick(\"PostCode\",\"Postcode\",\"Post_Code\",\"Post Code\") or deep_find_first_key(org, r\"post\\s*code|postcode\")\n",
    "    parts = [line1,line2,line3,line4,city,county,country]\n",
    "    addr_full = \", \".join([ensure_str(x).strip() for x in parts if x and str(x).strip()!=\"\"])\n",
    "    pc_compact, pc_spaced = clean_postcode(pc)\n",
    "    return {\n",
    "        \"AddrLine1\": ensure_str(line1), \"AddrLine2\": ensure_str(line2),\n",
    "        \"AddrLine3\": ensure_str(line3), \"AddrLine4\": ensure_str(line4),\n",
    "        \"TownCity\": ensure_str(city), \"County\": ensure_str(county), \"Country\": ensure_str(country),\n",
    "        \"PostCode\": pc_compact, \"PostCodeSpaced\": pc_spaced, \"AddressFull\": addr_full\n",
    "    }\n",
    "\n",
    "def extract_dates(org):\n",
    "    out=[]; d = org.get(\"Date\")\n",
    "    if isinstance(d, list):\n",
    "        for item in d:\n",
    "            if isinstance(item, dict):\n",
    "                out.append({\"DateType\": ensure_str(item.get(\"Type\")), \"Start\": ensure_str(item.get(\"Start\")), \"End\": ensure_str(item.get(\"End\"))})\n",
    "    return out\n",
    "\n",
    "def extract_roles(org):\n",
    "    out=[]; R = org.get(\"Roles\")\n",
    "    def emit(it):\n",
    "        if not isinstance(it, dict): return\n",
    "        rid = scalar(it.get(\"id\") or it.get(\"idCode\") or it.get(\"Id\") or it.get(\"code\"))\n",
    "        primary = bool(it.get(\"primaryRole\", False))\n",
    "        rstat = ensure_str(scalar(it.get(\"Status\")))\n",
    "        dates=[]; d = it.get(\"Date\")\n",
    "        if isinstance(d, list):\n",
    "            for di in d:\n",
    "                if isinstance(di, dict):\n",
    "                    dates.append({\"Type\": ensure_str(di.get(\"Type\")), \"Start\": ensure_str(di.get(\"Start\")), \"End\": ensure_str(di.get(\"End\"))})\n",
    "        out.append({\"RoleId\": ensure_str(rid), \"PrimaryRole\": primary, \"RoleStatus\": rstat, \"RoleDates\": dates})\n",
    "    if isinstance(R, list):\n",
    "        for it in R: emit(it)\n",
    "    elif isinstance(R, dict):\n",
    "        rl = R.get(\"Role\")\n",
    "        if isinstance(rl, list):\n",
    "            for it in rl: emit(it)\n",
    "        elif isinstance(rl, dict):\n",
    "            emit(rl)\n",
    "    return out\n",
    "\n",
    "def extract_rels(org):\n",
    "    out=[]; R = org.get(\"Rels\")\n",
    "    if not isinstance(R, dict): return out\n",
    "    rl = R.get(\"Rel\")\n",
    "    items = rl if isinstance(rl, list) else ([rl] if isinstance(rl, dict) else [])\n",
    "    for it in items:\n",
    "        if not isinstance(it, dict): continue\n",
    "        rel_id = ensure_str(scalar(it.get(\"id\"))); rstat  = ensure_str(scalar(it.get(\"Status\")))\n",
    "        target = it.get(\"Target\") or {}\n",
    "        tgt_org_raw = target.get(\"OrgId\")\n",
    "        tgt_org = ensure_str(tgt_org_raw.get(\"extension\") if isinstance(tgt_org_raw, dict) and \"extension\" in tgt_org_raw else scalar(tgt_org_raw))\n",
    "        tgt_role_raw = target.get(\"PrimaryRoleId\") or {}\n",
    "        tgt_role = ensure_str((tgt_role_raw.get(\"id\") if isinstance(tgt_role_raw, dict) else tgt_role_raw) or \"\")\n",
    "        dates=[]; d = it.get(\"Date\")\n",
    "        if isinstance(d, list):\n",
    "            for di in d:\n",
    "                if isinstance(di, dict):\n",
    "                    dates.append({\"Type\": ensure_str(di.get(\"Type\")), \"Start\": ensure_str(di.get(\"Start\")), \"End\": ensure_str(di.get(\"End\"))})\n",
    "        out.append({\"RelId\": rel_id, \"RelStatus\": rstat, \"TargetOrgId\": tgt_org, \"TargetPrimaryRoleId\": tgt_role, \"RelDates\": dates})\n",
    "    return out\n",
    "\n",
    "def extract_succs(org):\n",
    "    out=[]; S = org.get(\"Succs\")\n",
    "    if not isinstance(S, dict): return out\n",
    "    sc = S.get(\"Succ\")\n",
    "    items = sc if isinstance(sc, list) else ([sc] if isinstance(sc, dict) else [])\n",
    "    for it in items:\n",
    "        if not isinstance(it, dict): continue\n",
    "        typ = ensure_str(scalar(it.get(\"Type\"))); target = it.get(\"Target\") or {}\n",
    "        tgt_org_raw = target.get(\"OrgId\")\n",
    "        tgt_org = ensure_str(tgt_org_raw.get(\"extension\") if isinstance(tgt_org_raw, dict) and \"extension\" in tgt_org_raw else scalar(tgt_org_raw))\n",
    "        tgt_role_raw = target.get(\"PrimaryRoleId\") or {}\n",
    "        tgt_role = ensure_str((tgt_role_raw.get(\"id\") if isinstance(tgt_role_raw, dict) else tgt_role_raw) or \"\")\n",
    "        dates=[]; d = it.get(\"Date\")\n",
    "        if isinstance(d, list):\n",
    "            for di in d:\n",
    "                if isinstance(di, dict):\n",
    "                    dates.append({\"Type\": ensure_str(di.get(\"Type\")), \"Start\": ensure_str(di.get(\"Start\")), \"End\": ensure_str(di.get(\"End\"))})\n",
    "        out.append({\"SuccType\": typ, \"TargetOrgId\": tgt_org, \"TargetPrimaryRoleId\": tgt_role, \"SuccDates\": dates})\n",
    "    return out\n",
    "\n",
    "def make_clean_extracts():\n",
    "    # locate newest chunks from either baseline or sync\n",
    "    chunks_dirs = sorted(BRONZE_ROOT.glob(\"release_date=*/source=ord/*/dataset=*/chunks\"))\n",
    "    assert chunks_dirs, \"No chunks folder found.\"\n",
    "    latest_chunks = chunks_dirs[-1]\n",
    "\n",
    "    org_rows, date_rows, role_rows, rel_rows, succ_rows = [], [], [], [], []\n",
    "    files = list(latest_chunks.glob(\"org_*.json\"))\n",
    "\n",
    "    for fp in files:\n",
    "        j = read_json(fp, {})\n",
    "        org = pick_org(j)\n",
    "        oid = get_org_id(org)\n",
    "        name = ensure_str(scalar(org.get(\"Name\")))\n",
    "        status = ensure_str(scalar(org.get(\"Status\")))\n",
    "        lcd = ensure_str(scalar(org.get(\"LastChangeDate\")))\n",
    "        rc_code, rc_label = get_record_class(org)\n",
    "        addr = extract_address_fields(org)\n",
    "\n",
    "        org_rows.append({\n",
    "            \"OrgId\": oid,\n",
    "            \"Name\": name,\n",
    "            \"Status\": status,\n",
    "            \"IsActive\": (status.strip().lower()==\"active\"),\n",
    "            \"OrgRecordClass\": rc_code,\n",
    "            \"OrgRecordClassLabel\": rc_label,\n",
    "            \"LastChangeDate\": lcd,\n",
    "            **addr\n",
    "        })\n",
    "\n",
    "        for d in extract_dates(org):\n",
    "            date_rows.append({\"OrgId\": oid, **d})\n",
    "\n",
    "        for r in extract_roles(org):\n",
    "            if r[\"RoleDates\"]:\n",
    "                for rd in r[\"RoleDates\"]:\n",
    "                    role_rows.append({\n",
    "                        \"OrgId\": oid,\n",
    "                        \"RoleId\": r[\"RoleId\"],\n",
    "                        \"PrimaryRole\": r[\"PrimaryRole\"],\n",
    "                        \"RoleStatus\": r[\"RoleStatus\"],\n",
    "                        \"RoleDateType\": rd[\"Type\"],\n",
    "                        \"RoleStart\": rd[\"Start\"],\n",
    "                        \"RoleEnd\": rd[\"End\"],\n",
    "                    })\n",
    "            else:\n",
    "                role_rows.append({\n",
    "                    \"OrgId\": oid,\n",
    "                    \"RoleId\": r[\"RoleId\"],\n",
    "                    \"PrimaryRole\": r[\"PrimaryRole\"],\n",
    "                    \"RoleStatus\": r[\"RoleStatus\"],\n",
    "                    \"RoleDateType\": \"\",\n",
    "                    \"RoleStart\": \"\",\n",
    "                    \"RoleEnd\": \"\",\n",
    "                })\n",
    "\n",
    "        for rel in extract_rels(org):\n",
    "            if rel[\"RelDates\"]:\n",
    "                for rd in rel[\"RelDates\"]:\n",
    "                    rel_rows.append({\n",
    "                        \"OrgId\": oid,\n",
    "                        \"RelId\": rel[\"RelId\"],\n",
    "                        \"RelStatus\": rel[\"RelStatus\"],\n",
    "                        \"TargetOrgId\": rel[\"TargetOrgId\"],\n",
    "                        \"TargetPrimaryRoleId\": rel[\"TargetPrimaryRoleId\"],\n",
    "                        \"RelDateType\": rd[\"Type\"],\n",
    "                        \"RelStart\": rd[\"Start\"],\n",
    "                        \"RelEnd\": rd[\"End\"],\n",
    "                    })\n",
    "            else:\n",
    "                rel_rows.append({\n",
    "                    \"OrgId\": oid,\n",
    "                    \"RelId\": rel[\"RelId\"],\n",
    "                    \"RelStatus\": rel[\"RelStatus\"],\n",
    "                    \"TargetOrgId\": rel[\"TargetOrgId\"],\n",
    "                    \"TargetPrimaryRoleId\": rel[\"TargetPrimaryRoleId\"],\n",
    "                    \"RelDateType\": \"\",\n",
    "                    \"RelStart\": \"\",\n",
    "                    \"RelEnd\": \"\",\n",
    "                })\n",
    "\n",
    "        for sc in extract_succs(org):\n",
    "            if sc[\"SuccDates\"]:\n",
    "                for sd in sc[\"SuccDates\"]:\n",
    "                    succ_rows.append({\n",
    "                        \"OrgId\": oid,\n",
    "                        \"SuccType\": sc[\"SuccType\"],\n",
    "                        \"TargetOrgId\": sc[\"TargetOrgId\"],\n",
    "                        \"TargetPrimaryRoleId\": sc[\"TargetPrimaryRoleId\"],\n",
    "                        \"SuccDateType\": sd[\"Type\"],\n",
    "                        \"SuccStart\": sd[\"Start\"],\n",
    "                        \"SuccEnd\": sd[\"End\"],\n",
    "                    })\n",
    "            else:\n",
    "                succ_rows.append({\n",
    "                    \"OrgId\": oid,\n",
    "                    \"SuccType\": sc[\"SuccType\"],\n",
    "                    \"TargetOrgId\": sc[\"TargetOrgId\"],\n",
    "                    \"TargetPrimaryRoleId\": sc[\"TargetPrimaryRoleId\"],\n",
    "                    \"SuccDateType\": \"\",\n",
    "                    \"SuccStart\": \"\",\n",
    "                    \"SuccEnd\": \"\",\n",
    "                })\n",
    "\n",
    "    # ---- safe builders (handle empty tables + missing cols) ----\n",
    "    def build_df(rows, required_cols, sort_cols):\n",
    "        df = pd.DataFrame(rows)\n",
    "        if df.empty:\n",
    "            df = pd.DataFrame(columns=required_cols)\n",
    "        else:\n",
    "            for c in required_cols:\n",
    "                if c not in df.columns:\n",
    "                    df[c] = \"\"\n",
    "        sort_by = [c for c in sort_cols if c in df.columns]\n",
    "        if sort_by:\n",
    "            df = df.sort_values(sort_by)\n",
    "        return df.reset_index(drop=True)\n",
    "\n",
    "    orgs_df  = build_df(\n",
    "        org_rows,\n",
    "        required_cols=[\"OrgId\",\"Name\",\"Status\",\"IsActive\",\"OrgRecordClass\",\"OrgRecordClassLabel\",\n",
    "                       \"LastChangeDate\",\"AddrLine1\",\"AddrLine2\",\"AddrLine3\",\"AddrLine4\",\n",
    "                       \"TownCity\",\"County\",\"Country\",\"PostCode\",\"PostCodeSpaced\",\"AddressFull\"],\n",
    "        sort_cols=[\"OrgId\"]\n",
    "    ).drop_duplicates(subset=[\"OrgId\"])\n",
    "\n",
    "    dates_df = build_df(\n",
    "        date_rows,\n",
    "        required_cols=[\"OrgId\",\"DateType\",\"Start\",\"End\"],\n",
    "        sort_cols=[\"OrgId\",\"DateType\",\"Start\"]\n",
    "    )\n",
    "\n",
    "    roles_df = build_df(\n",
    "        role_rows,\n",
    "        required_cols=[\"OrgId\",\"RoleId\",\"PrimaryRole\",\"RoleStatus\",\"RoleDateType\",\"RoleStart\",\"RoleEnd\"],\n",
    "        sort_cols=[\"OrgId\",\"RoleId\",\"RoleStart\"]\n",
    "    )\n",
    "\n",
    "    rels_df = build_df(\n",
    "        rel_rows,\n",
    "        required_cols=[\"OrgId\",\"RelId\",\"RelStatus\",\"TargetOrgId\",\"TargetPrimaryRoleId\",\n",
    "                       \"RelDateType\",\"RelStart\",\"RelEnd\"],\n",
    "        sort_cols=[\"OrgId\",\"RelId\",\"RelStart\"]\n",
    "    )\n",
    "\n",
    "    succ_df = build_df(\n",
    "        succ_rows,\n",
    "        required_cols=[\"OrgId\",\"SuccType\",\"TargetOrgId\",\"TargetPrimaryRoleId\",\"SuccDateType\",\"SuccStart\",\"SuccEnd\"],\n",
    "        sort_cols=[\"OrgId\",\"SuccType\",\"SuccStart\"]\n",
    "    )\n",
    "\n",
    "    # ---- write extracts ----\n",
    "    extracts = BRONZE_ROOT / \"extracts\"\n",
    "    ensure_dir(extracts)\n",
    "    ts = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    orgs_df.to_csv(extracts / f\"orgs_{ts}.csv\", index=False)\n",
    "    orgs_df.to_parquet(extracts / f\"orgs_{ts}.parquet\", index=False)\n",
    "    dates_df.to_csv(extracts / f\"org_dates_{ts}.csv\", index=False)\n",
    "    roles_df.to_csv(extracts / f\"org_roles_{ts}.csv\", index=False)\n",
    "    rels_df.to_csv(extracts / f\"org_rels_{ts}.csv\", index=False)\n",
    "    succ_df.to_csv(extracts / f\"org_succs_{ts}.csv\", index=False)\n",
    "\n",
    "    print(\"Wrote tidy extracts to:\", extracts)\n",
    "    display(orgs_df.head(10))\n",
    "    return orgs_df, roles_df, rels_df, succ_df\n",
    "\n",
    "\n",
    "# ------------------- SMART RUNNER -------------------\n",
    "def smart_run():\n",
    "    \"\"\"Prefer /sync if watermark exists; else baseline. Never re-download existing orgs.\"\"\"\n",
    "    ensure_dir(BRONZE_ROOT)\n",
    "    print(\"Bronze root:\", BRONZE_ROOT.resolve())\n",
    "\n",
    "    # If ROLE_IDS is None, discover from /roles\n",
    "    global ROLE_IDS\n",
    "    if ROLE_IDS is None:\n",
    "        print(\"Discovering all Role IDs from /roles ...\")\n",
    "        ROLE_IDS = discover_role_ids()\n",
    "        print(f\"Discovered {len(ROLE_IDS)} roles\")\n",
    "\n",
    "    wm = get_wm()\n",
    "    if wm.get(\"ord_last_change_date\"):\n",
    "        print(\"Watermark found → running INCREMENTAL SYNC (new/changed only)\")\n",
    "        sync_dir = incremental_sync_with_progress()\n",
    "        return sync_dir\n",
    "    else:\n",
    "        print(\"No watermark → running BASELINE (deduped; skips already-downloaded orgs)\")\n",
    "        base_dir = baseline_roles_with_progress(ROLE_IDS)\n",
    "        return base_dir\n",
    "\n",
    "# ------------------- RUN -------------------\n",
    "out_dir = smart_run()\n",
    "\n",
    "if MAKE_CLEAN_EXTRACTS:\n",
    "    orgs_df, roles_df, rels_df, succ_df = make_clean_extracts()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
